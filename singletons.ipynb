{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signal(w,theta,n):\n",
    "    \"\"\"\n",
    "    Assumes normalized amplitude\n",
    "    \"\"\"\n",
    "    t = np.arange(n)\n",
    "    signal = np.exp(1j*(w*t + theta))\n",
    "    return signal\n",
    "\n",
    "def make_noise(sigma2,n):\n",
    "    noise_scaling = np.sqrt(sigma2/2)\n",
    "    # noise is complex valued\n",
    "    noise  = noise_scaling*np.random.randn(n) + 1j*noise_scaling*np.random.randn(n)\n",
    "    return noise\n",
    "\n",
    "def make_noisy_signal(w,theta,SNRdb,n):\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdb)\n",
    "    signal = make_signal(w,theta,n)\n",
    "    noise  = make_noise(sigma2,n)\n",
    "    return signal + noise\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "def make_batch_noisy(batch_size, SNRdb, N, m, binary=False):\n",
    "    signals, freqs = [], []\n",
    "    for i in range(batch_size):\n",
    "        freq = np.random.randint(0, N)\n",
    "        w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "        sig = make_noisy_signal(w, 0, SNRdb, m)\n",
    "        signals.append(sig)\n",
    "        freqs.append(freq)\n",
    "    if binary:\n",
    "        return signals, make_binary(freqs, N), one_hot(N, batch_size, freqs)\n",
    "    return signals, one_hot(N, batch_size, freqs)\n",
    "\n",
    "def make_batch_singleton(batch_size, SNRdb, N, m, default=-1): # 0 = zero, 1 = single, 2 = multi\n",
    "    signals, freqs = [], []\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdB)\n",
    "    for i in range(batch_size):\n",
    "        val = np.random.poisson(0.79)\n",
    "        if default >= 0:\n",
    "            val = default\n",
    "        if val == 0:\n",
    "            signals.append(make_noise(0, m))\n",
    "            freqs.append([1, 0, 0])\n",
    "        if val == 1:\n",
    "            signals.append(make_noisy_signal(2 * np.pi * np.random.randint(0, N) / N, 0, SNRdB, m))\n",
    "            freqs.append([0, 1, 0])\n",
    "        if val >= 2:\n",
    "            signal = make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            for i in range(val - 1):\n",
    "                signal += make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            signals.append(signal + make_noise(sigma2, m))\n",
    "            freqs.append([0, 0, 1])\n",
    "    return signals, freqs\n",
    "\n",
    "def get_sigma2_from_snrdb(SNR_db):\n",
    "    return 10**(-SNR_db/10)\n",
    "\n",
    "def kay_weights(N):\n",
    "    scaling = (3.0/2)*N/(N**2 - 1)\n",
    "    \n",
    "    w = [1 - ((i - (N/2 - 1))/(N/2))**2 for i in range(N-1)]\n",
    "    \n",
    "    return scaling*np.array(w)\n",
    "\n",
    "def kays_method(my_signal):\n",
    "    N = len(my_signal)\n",
    "    w = kay_weights(N)\n",
    "    \n",
    "    angle_diff = np.angle(np.conj(my_signal[0:-1])*my_signal[1:])\n",
    "    need_to_shift = np.any(angle_diff < -np.pi/2)\n",
    "    if need_to_shift:    \n",
    "        neg_idx = angle_diff < 0\n",
    "        angle_diff[neg_idx] += np.pi*2\n",
    "    \n",
    "    return w.dot(angle_diff)\n",
    "\n",
    "def kays_singleton_accuracy(test_signals, test_freqs, N):\n",
    "    diffs = [s - make_signal(kays_method(s), 0, N) for s in test_signals]\n",
    "    thresh, single_acc, other_acc, best_thresh = 0.0, 0, 0, 0\n",
    "    best = 0\n",
    "    for i in range(150):\n",
    "        vals = [(sum(np.absolute(s)) / N) < thresh for s in diffs]\n",
    "        corr = [1 for i in range(len(test_freqs)) if (test_freqs[i] == [0, 1, 0] and vals[i] == 1) or ((test_freqs[i] != [0, 1, 0] and vals[i] == 0))]\n",
    "        corr = sum(corr)\n",
    "        #single = sum([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]])\n",
    "        #other = sum([not vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]])        \n",
    "        #if single*2 + other > single_acc*2 + other_acc and single > 0.2 and other > 0.2:\n",
    "        #    single_acc = single\n",
    "        #    other_acc = other\n",
    "        #    best_thresh = thresh\n",
    "        if corr > best:\n",
    "            best = corr\n",
    "            best_thresh = thresh\n",
    "        thresh += 0.05\n",
    "    print('thresh: ', best_thresh)\n",
    "    return best / len(test_signals)\n",
    "\n",
    "def test_kays(signals, freqs, N):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        res = kays_method(sig)\n",
    "        res = round(res * N / (2 * np.pi))\n",
    "        if np.argmax(freq) == res:\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "\n",
    "def test_mle(signals, freqs, N, m):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        cleans = [make_signal(np.pi * 2 * w / N, 0, m) for w in range(N)]\n",
    "        dots = [np.absolute(np.vdot(sig, clean)) for clean in cleans]\n",
    "        if np.argmax(dots) == np.argmax(freq):\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "    \n",
    "def make_binary(freqs, N):\n",
    "    w = math.ceil(np.log2(N))\n",
    "    return [[int(a) for a in list(np.binary_repr(f, width=w))] for f in freqs] \n",
    "\n",
    "def binary_to_int(binary_string):\n",
    "    return tf.reduce_sum(\n",
    "    tf.cast(tf.reverse(tensor=binary_string, axis=[0]), dtype=tf.int64)\n",
    "    * 2 ** tf.range(tf.cast(tf.size(binary_string), dtype=tf.int64)))\n",
    "    '''y = 0\n",
    "    for i,j in enumerate(x):\n",
    "        y += j<<i\n",
    "    return y'''\n",
    "\n",
    "def hamming(pred, act):\n",
    "    return np.count_nonzero(pred != act)\n",
    "\n",
    "def one_hot(N, batch_size, freqs):\n",
    "    freqs_one_hot = np.zeros((batch_size, N))\n",
    "    freqs_one_hot[np.arange(batch_size), freqs] = 1\n",
    "    return freqs_one_hot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 0.999\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 0.999\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 0.999\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 0.999\n",
      "Testing Accuracy NonSingelton: 0.999\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 1.0\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 0.999\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 1.0\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 0.0\n",
      "Testing Accuracy NonSingelton: 0.5\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 1.0\n",
      "Training Finished for snr= 8\n",
      "Testing Accuracy Sing: 1.0\n",
      "Testing Accuracy NonSingelton: 0.998\n"
     ]
    }
   ],
   "source": [
    "# seeing snr vs accuracy for singleton detection (kay vs nn)\n",
    "\n",
    "nn_accs = []\n",
    "nn_accs2 = []\n",
    "kays = []\n",
    "kay2s = []\n",
    "snrs = [8, 6, 4, 2, 0, -2, -4, -6]\n",
    "\n",
    "for SNRdB in snrs:\n",
    "    \n",
    "    trial_nn = []\n",
    "    trial_nn2 = []\n",
    "    trial_kay = []\n",
    "    trial_kay2 = []\n",
    "    for _ in range(10):\n",
    "\n",
    "        N = 27000 #512\n",
    "        #SNRdB = 0\n",
    "        m = 300 #16\n",
    "\n",
    "        # Parameters\n",
    "        learning_rate = 0.005\n",
    "        num_iter = 8000\n",
    "        batch_size = 1000\n",
    "\n",
    "        # Network Parameters\n",
    "        num_classes = 3\n",
    "\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, m, 2])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([5, 2, 2])), # filtersize, in channels, outchannels\n",
    "            'out': tf.Variable(tf.random_normal([(m-4-2-2)*2, num_classes])),\n",
    "            'h2': tf.Variable(tf.random_normal([3, 2, 2])),\n",
    "            'h3': tf.Variable(tf.random_normal([3, 2, 2]))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([2])),\n",
    "            'out': tf.Variable(tf.random_normal([num_classes])),\n",
    "            'b2': tf.Variable(tf.random_normal([2])),\n",
    "            'b3': tf.Variable(tf.random_normal([2]))\n",
    "        }\n",
    "\n",
    "        '''test_signals, test_freqs = make_batch_singleton(batch_size, SNRdB, N, m)\n",
    "        test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "        test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "        test_signals_pair[:, :, 1] = np.imag(test_signals)'''\n",
    "        \n",
    "        test_signals, test_freqs = make_batch_singleton(batch_size, SNRdB, N, m, default=1)\n",
    "        test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "        test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "        test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "        \n",
    "        test_signals2, test_freqs2 = make_batch_singleton(batch_size//2, SNRdB, N, m, default=0)\n",
    "        test_signals3, test_freqs3 = make_batch_singleton(batch_size//2, SNRdB, N, m, default=2)\n",
    "        test_signals2.extend(test_signals3)\n",
    "        test_freqs2.extend(test_freqs3)\n",
    "        test_signals_pair2 = np.zeros((batch_size, m, 2))\n",
    "        test_signals_pair2[:, :, 0] = np.real(test_signals2)\n",
    "        test_signals_pair2[:, :, 1] = np.imag(test_signals2)\n",
    "        training_size = 500\n",
    "        dict = {}\n",
    "        for i in range(training_size):\n",
    "            batch_x, batch_y = make_batch_singleton(batch_size, SNRdB, N, m)\n",
    "            batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "            batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "            batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "            dict[i] = (batch_x_pair, batch_y)\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights['h1'], 1, 'VALID'), biases['b1'])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            layer_2 = tf.add(tf.nn.conv1d(hidden_1, weights['h2'], 1, 'VALID'), biases['b2'])\n",
    "            hidden_2 = tf.nn.relu(layer_2)\n",
    "            layer_3 = tf.add(tf.nn.conv1d(hidden_2, weights['h3'], 1, 'VALID'), biases['b3'])\n",
    "            hidden_3 = tf.nn.relu(layer_3)\n",
    "            hidden_3 = tf.reshape(hidden_3, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net(X)\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))  \n",
    "        '''+ 0.01*tf.nn.l2_loss(weights['h1']) + 0.01*tf.nn.l2_loss(weights['h2']) + 0.01*tf.nn.l2_loss(weights['out']) \\\n",
    "        + 0.01*tf.nn.l2_loss(biases['b1']) + 0.01*tf.nn.l2_loss(biases['b2']) + 0.01*tf.nn.l2_loss(biases['out']) '''\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "\n",
    "            for step in range(1, num_iter + 1):\n",
    "                batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "                if step % 500 == 0:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "\n",
    "                    #print(\"pred: \", [np.argmax(a) for a in pred[:8]])\n",
    "                    #print(\"act:\", [np.argmax(a) for a in batch_y[:8]])\n",
    "                    accuracies.append(acc)\n",
    "                    losses.append(loss)\n",
    "                    #print(\"snr: \", SNRdB)\n",
    "                    #print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                    #      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    #      \"{:.3f}\".format(acc))\n",
    "            print(\"Training Finished for snr=\", SNRdB)\n",
    "\n",
    "            nn_acc = sess.run(accuracy, feed_dict={X: test_signals_pair, Y: test_freqs})  # only singletons   \n",
    "            nn_acc2 = sess.run(accuracy, feed_dict={X: test_signals_pair2, Y: test_freqs2})  # only nonsingletons\n",
    "            print(\"Testing Accuracy Sing:\", nn_acc)\n",
    "            print(\"Testing Accuracy NonSingelton:\", nn_acc2)\n",
    "            #kay_acc = kays_singleton_accuracy(test_signals, test_freqs, m) # only singletons\n",
    "            #kay_acc2 = kays_singleton_accuracy(test_signals, test_freqs, m) # only nonsingletons\n",
    "            #print(\"Testing Accuracy Kay Sing:\", kay_acc)\n",
    "            #print(\"Testing Accuracy Kay NonSing:\", kay_acc2)\n",
    "            trial_nn.append(nn_acc)\n",
    "            trial_nn2.append(nn_acc2)\n",
    "            #trial_kay.append(kay_acc)\n",
    "            #trial_kay2.append(kay_acc2)\n",
    "    nn_accs.append(np.median(trial_nn))\n",
    "    nn_accs2.append(np.median(trial_nn2))\n",
    "    #kays.append(np.median(trial_kay))\n",
    "    #kay2s.append(np.median(trial_kay2))\n",
    "        \n",
    "np.save('./data/singleton_conf/snrs', snrs)\n",
    "np.save('./data/singleton_conf/nn_single', nn_accs)\n",
    "np.save('./data/singleton_conf/nn_nonsingle', nn_accs2)\n",
    "#np.save('./data/singleton_conf/kay_single', kays)\n",
    "#np.save('./data/singleton_conf/kay_nonsingle', kay2s)\n",
    "plt.plot(snrs, nn_accs, '--bo')\n",
    "plt.plot(snrs, nn_accs2, '--ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.902\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.887\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.89\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.913\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.877\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.882\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.566\n",
      "Training Finished for layers= 1\n",
      "Testing Accuracy Neural: 0.854\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.913\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.872\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.816\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 3\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.823\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 5\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.829\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.813\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 7\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.816\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.816\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.875\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 9\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.922\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n",
      "Training Finished for layers= 11\n",
      "Testing Accuracy Neural: 0.468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23bb8371390>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# seeing num layers vs accuracy \n",
    "\n",
    "nn_accs = []\n",
    "layers = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "N = 27000 \n",
    "SNRdB = -8\n",
    "m = 300 \n",
    "\n",
    "learning_rate = 0.005\n",
    "num_iter = 8000\n",
    "batch_size = 1000\n",
    "\n",
    "test_signals, test_freqs = make_batch_singleton(batch_size, SNRdB, N, m)\n",
    "test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "training_size = 500\n",
    "\n",
    "dict = {}\n",
    "for i in range(training_size):\n",
    "    batch_x, batch_y = make_batch_singleton(batch_size, SNRdB, N, m)\n",
    "    batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "    batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "    batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "    dict[i] = (batch_x_pair, batch_y)\n",
    "\n",
    "for layer in layers:\n",
    "    \n",
    "    trial = []\n",
    "    for _ in range(10):\n",
    "\n",
    "        # Network Parameters\n",
    "        num_classes = 3\n",
    "\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, m, 2])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        # Store layers weight & bias\n",
    "        weights = {i: tf.Variable(tf.random_normal([3, 2, 2])) for i in range(1, layer+1)}\n",
    "        weights[0] = tf.Variable(tf.random_normal([5, 2, 2]))\n",
    "        weights['out'] = tf.Variable(tf.random_normal([(m-4-(2*layer))*2, num_classes]))\n",
    "        biases = {i: tf.Variable(tf.random_normal([2])) for i in range(layer+1)}\n",
    "        biases['out'] = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights[0], 1, 'VALID'), biases[0])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            for i in range(1, layer+1):\n",
    "                layer_1 = tf.add(tf.nn.conv1d(hidden_1, weights[i], 1, 'VALID'), biases[i])\n",
    "                hidden_1 = tf.nn.relu(layer_1)\n",
    "            hidden_3 = tf.reshape(hidden_1, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net(X)\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))  \n",
    "        '''+ 0.01*tf.nn.l2_loss(weights['h1']) + 0.01*tf.nn.l2_loss(weights['h2']) + 0.01*tf.nn.l2_loss(weights['out']) \\\n",
    "        + 0.01*tf.nn.l2_loss(biases['b1']) + 0.01*tf.nn.l2_loss(biases['b2']) + 0.01*tf.nn.l2_loss(biases['out']) '''\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "\n",
    "            for step in range(1, num_iter + 1):\n",
    "                batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "                if step % 500 == 0:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "\n",
    "                    #print(\"pred: \", [np.argmax(a) for a in pred[:8]])\n",
    "                    #print(\"act:\", [np.argmax(a) for a in batch_y[:8]])\n",
    "                    accuracies.append(acc)\n",
    "                    losses.append(loss)\n",
    "                    #print(\"snr: \", SNRdB)\n",
    "                    #print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                    #      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    #      \"{:.3f}\".format(acc))\n",
    "            print(\"Training Finished for layers=\", layer)\n",
    "\n",
    "            nn_acc = sess.run(accuracy, feed_dict={X: test_signals_pair, Y: test_freqs})        \n",
    "            print(\"Testing Accuracy Neural:\", nn_acc)\n",
    "            trial.append(nn_acc)\n",
    "    nn_accs.append(np.median(trial))\n",
    "np.save('./data/singleton/layers', layers)\n",
    "np.save('./data/singleton/layer_accs', nn_accs)\n",
    "plt.plot(layers, nn_accs, '--bo')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.903\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.943\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.887\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.93\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.442\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.442\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.443\n",
      "Training Finished for dataset snr= -4\n",
      "Testing Accuracy Neural: 0.944\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.955\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.951\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.933\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.442\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.442\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.934\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.442\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -6\n",
      "Testing Accuracy Neural: 0.442\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.963\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.9\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.936\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.444\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.375\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.944\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.957\n",
      "Training Finished for dataset snr= -8\n",
      "Testing Accuracy Neural: 0.96\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.902\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.911\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.896\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.863\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.375\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.864\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.885\n",
      "Training Finished for dataset snr= -10\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.957\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.959\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.935\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.954\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.967\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.941\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.817\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.874\n",
      "Training Finished for dataset snr= -12\n",
      "Testing Accuracy Neural: 0.442\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# seeing training on lower vs same snr\n",
    "\n",
    "nn_accs = []\n",
    "\n",
    "N = 27000 \n",
    "SNRdB = -6\n",
    "m = 300 \n",
    "\n",
    "learning_rate = 0.005\n",
    "num_iter = 8000\n",
    "batch_size = 1000\n",
    "\n",
    "test_signals, test_freqs = make_batch_singleton(batch_size, SNRdB, N, m)\n",
    "test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "\n",
    "snrs = [-4, -6, -8, -10, -12]\n",
    "\n",
    "for snr in snrs:\n",
    "\n",
    "    trial = []\n",
    "    for _ in range(10):\n",
    "        # Network Parameters\n",
    "        num_classes = 3\n",
    "\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, m, 2])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([5, 2, 2])), # filtersize, in channels, outchannels\n",
    "            'out': tf.Variable(tf.random_normal([(m-4-2-2)*2, num_classes])),\n",
    "            'h2': tf.Variable(tf.random_normal([3, 2, 2])),\n",
    "            'h3': tf.Variable(tf.random_normal([3, 2, 2]))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([2])),\n",
    "            'out': tf.Variable(tf.random_normal([num_classes])),\n",
    "            'b2': tf.Variable(tf.random_normal([2])),\n",
    "            'b3': tf.Variable(tf.random_normal([2]))\n",
    "        }\n",
    "\n",
    "        dict = {}\n",
    "        training_size = 500\n",
    "        for i in range(training_size):\n",
    "            batch_x, batch_y = make_batch_singleton(batch_size, snr, N, m)\n",
    "            batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "            batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "            batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "            dict[i] = (batch_x_pair, batch_y)\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights['h1'], 1, 'VALID'), biases['b1'])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            layer_2 = tf.add(tf.nn.conv1d(hidden_1, weights['h2'], 1, 'VALID'), biases['b2'])\n",
    "            hidden_2 = tf.nn.relu(layer_2)\n",
    "            layer_3 = tf.add(tf.nn.conv1d(hidden_2, weights['h3'], 1, 'VALID'), biases['b3'])\n",
    "            hidden_3 = tf.nn.relu(layer_3)\n",
    "            hidden_3 = tf.reshape(hidden_3, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net(X)\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))  \n",
    "        '''+ 0.01*tf.nn.l2_loss(weights['h1']) + 0.01*tf.nn.l2_loss(weights['h2']) + 0.01*tf.nn.l2_loss(weights['out']) \\\n",
    "        + 0.01*tf.nn.l2_loss(biases['b1']) + 0.01*tf.nn.l2_loss(biases['b2']) + 0.01*tf.nn.l2_loss(biases['out']) '''\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "\n",
    "            for step in range(1, num_iter + 1):\n",
    "                batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "                if step % 500 == 0:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "\n",
    "                    #print(\"pred: \", [np.argmax(a) for a in pred[:8]])\n",
    "                    #print(\"act:\", [np.argmax(a) for a in batch_y[:8]])\n",
    "                    accuracies.append(acc)\n",
    "                    losses.append(loss)\n",
    "                    #print(\"snr: \", SNRdB)\n",
    "                    #print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                    #      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    #      \"{:.3f}\".format(acc))\n",
    "            print(\"Training Finished for dataset snr=\", snr)\n",
    "\n",
    "            nn_acc = sess.run(accuracy, feed_dict={X: test_signals_pair, Y: test_freqs})        \n",
    "            print(\"Testing Accuracy Neural:\", nn_acc)\n",
    "            trial.append(nn_acc)\n",
    "    nn_accs.append(np.median(trial))\n",
    "np.save('./data/singleton/snrs_training', snrs)\n",
    "np.save('./data/singleton/nn_accs_snrs_training', nn_accs)\n",
    "plt.plot(snrs, nn_accs, '--bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn binary\n",
    "\n",
    "nn_accs = []\n",
    "kay_accs = []\n",
    "\n",
    "N = 512\n",
    "SNRdB = 5\n",
    "m = 32\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "num_iter = 8000\n",
    "batch_size = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_classes = math.ceil(np.log2(N))\n",
    "\n",
    "layers = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "training_size = 1000\n",
    "dict = {}\n",
    "for i in range(training_size):\n",
    "    batch_x, batch_y, _ = make_batch_noisy(batch_size, SNRdB, N, m, True)\n",
    "    batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "    batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "    batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "    dict[i] = (batch_x_pair, batch_y)\n",
    "    \n",
    "test_signals, test_freqs, test_freqs_onehot = make_batch_noisy(batch_size, SNRdB, N, m, True)\n",
    "test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "\n",
    "\n",
    "for layer in layers:\n",
    "\n",
    "    trial = []\n",
    "    for _ in range(10):\n",
    "\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, m, 2])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        # Store layers weight & bias\n",
    "\n",
    "\n",
    "\n",
    "        # Store layers weight & bias\n",
    "        weights = {i: tf.Variable(tf.random_normal([3, 2, 2])) for i in range(1, layer+1)}\n",
    "        weights[0] = tf.Variable(tf.random_normal([5, 2, 2]))\n",
    "        weights['out'] = tf.Variable(tf.random_normal([(m-4-(2*layer))*2, num_classes]))\n",
    "        biases = {i: tf.Variable(tf.random_normal([2])) for i in range(layer+1)}\n",
    "        biases['out'] = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights[0], 1, 'VALID'), biases[0])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            for i in range(1, layer+1):\n",
    "                layer_1 = tf.add(tf.nn.conv1d(hidden_1, weights[i], 1, 'VALID'), biases[i])\n",
    "                hidden_1 = tf.nn.relu(layer_1)\n",
    "            hidden_3 = tf.reshape(hidden_1, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net(X)\n",
    "        prediction = tf.round(tf.nn.sigmoid(logits))\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))  \n",
    "        ''' + 0.01*tf.nn.l2_loss(weights['h1']) + 0.01*tf.nn.l2_loss(weights['h2']) + 0.01*tf.nn.l2_loss(weights['out'])\\\n",
    "        + 0.01*tf.nn.l2_loss(biases['b1']) + 0.01*tf.nn.l2_loss(biases['b2']) + 0.01*tf.nn.l2_loss(biases['out'])'''\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "        correct_pred = tf.equal(prediction, Y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "\n",
    "            for step in range(1, num_iter + 1):\n",
    "                batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "                if step % 100 == 0:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, pred = sess.run([loss_op, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "                    losses.append(loss)\n",
    "                '''if step % 50000 == 0:\n",
    "                    pred = sess.run(prediction, feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "                    preds = [binary_to_int(a).eval() for a in pred]\n",
    "                    acts = [binary_to_int(a).eval() for a in batch_y]\n",
    "                    print('batch accuracy: ', np.mean(np.equal(preds, acts)))\n",
    "                    pred = sess.run(prediction, feed_dict={X: test_signals_pair, Y: test_freqs})\n",
    "                    preds = [binary_to_int(a).eval() for a in pred]\n",
    "                    acts = [binary_to_int(a).eval() for a in test_freqs]\n",
    "                    print('test accuracy: ', np.mean(np.equal(preds, acts)))'''\n",
    "\n",
    "            #print(\"Training Finished\")\n",
    "\n",
    "            loss, nn_acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: test_signals_pair, Y: test_freqs})\n",
    "            #loss, nn_acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: full_sig_pair, Y: full_bin})\n",
    "            #hammings = [hamming(a, b) for a, b in zip(pred, test_freqs)]\n",
    "            #hamming_list.append(hammings)\n",
    "            #print(np.mean(hammings))\n",
    "            #plt.hist(hammings)\n",
    "            #plt.show()\n",
    "            preds = [binary_to_int(a).eval() for a in pred]\n",
    "            acts = [binary_to_int(a).eval() for a in test_freqs] \n",
    "            nn_acc = np.mean(np.equal(preds, acts))\n",
    "\n",
    "            #kay_acc = test_kays(test_signals, test_freqs, N)\n",
    "\n",
    "            print(\"Testing Accuracy Neural:\", nn_acc)\n",
    "\n",
    "            #print(\"Testing Accuracy Kay:\", kay_acc)\n",
    "            trial.append(nn_acc)\n",
    "            #kay_accs.append(kay_acc)\n",
    "    nn_accs.append(np.median(trial))\n",
    "np.save('./data/singleton/freq_layers', layers)\n",
    "np.save('./data/singleton/freq_accs_layers', nn_accs)\n",
    "plt.plot(layers, nn_accs, '--bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
