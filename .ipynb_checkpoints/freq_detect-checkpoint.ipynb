{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signal(w,theta,n):\n",
    "    \"\"\"\n",
    "    Assumes normalized amplitude\n",
    "    \"\"\"\n",
    "    t = np.arange(n)\n",
    "    signal = np.exp(1j*(w*t + theta))\n",
    "    return signal\n",
    "\n",
    "def make_noise(sigma2,n):\n",
    "    noise_scaling = np.sqrt(sigma2/2)\n",
    "    # noise is complex valued\n",
    "    noise  = noise_scaling*np.random.randn(n) + 1j*noise_scaling*np.random.randn(n)\n",
    "    return noise\n",
    "\n",
    "def make_noisy_signal(w,theta,SNRdb,n):\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdb)\n",
    "    signal = make_signal(w,theta,n)\n",
    "    noise  = make_noise(sigma2,n)\n",
    "    return signal + noise\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "def make_batch_noisy(batch_size, SNRdb, N, m, binary=False):\n",
    "    signals, freqs = [], []\n",
    "    for i in range(batch_size):\n",
    "        freq = np.random.randint(0, N)\n",
    "        w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "        sig = make_noisy_signal(w, 0, SNRdb, m)\n",
    "        signals.append(sig)\n",
    "        freqs.append(freq)\n",
    "    if binary:\n",
    "        return signals, make_binary(freqs, N), one_hot(N, batch_size, freqs)\n",
    "    return signals, one_hot(N, batch_size, freqs)\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "def make_batch_noisy_lohi(batch_size, SNRdb, N, m):\n",
    "    freqs = []\n",
    "    freqs.append(np.random.randint(0, N))\n",
    "    test_signals, test_freqs = make_noisy_lohi(SNRdB, N, m, freqs[-1])\n",
    "    for i in range(1, batch_size):\n",
    "        freqs.append(np.random.randint(0, N))\n",
    "        a, b = make_noisy_lohi(SNRdB, N, m, freqs[-1])\n",
    "        test_signals.extend(a)\n",
    "        test_freqs.extend(b)\n",
    "    return test_signals, test_freqs, freqs\n",
    "\n",
    "def make_noisy_lohi(SNRdb, N, m, freq):\n",
    "    signals, vals = [], []\n",
    "    steps = int(np.log2(N))\n",
    "    w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "    sig = make_noisy_signal(w, 0, SNRdb, m * (2**steps))\n",
    "    for i in range(int(np.log2(N))):\n",
    "        signals.append([sig[a * (2**i)] for a in range(m)])\n",
    "        if (freq * (2**i)) % (N) < N / 2:\n",
    "            vals.append([1, 0])\n",
    "        else:\n",
    "            vals.append([0, 1])\n",
    "    return signals, vals\n",
    "        \n",
    "\n",
    "def make_batch_singleton(batch_size, SNRdb, N, m, default=-1): # 0 = zero, 1 = single, 2 = multi\n",
    "    signals, freqs = [], []\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdB)\n",
    "    for i in range(batch_size):\n",
    "        val = np.random.poisson(0.79)\n",
    "        if default >= 0:\n",
    "            val = default\n",
    "        if val == 0:\n",
    "            signals.append(make_noise(0, m))\n",
    "            freqs.append([1, 0, 0])\n",
    "        if val == 1:\n",
    "            signals.append(make_noisy_signal(2 * np.pi * np.random.randint(0, N) / N, 0, SNRdB, m))\n",
    "            freqs.append([0, 1, 0])\n",
    "        if val >= 2:\n",
    "            signal = make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            for i in range(val - 1):\n",
    "                signal += make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            signals.append(signal + make_noise(sigma2, m))\n",
    "            freqs.append([0, 0, 1])\n",
    "    return signals, freqs\n",
    "\n",
    "def get_sigma2_from_snrdb(SNR_db):\n",
    "    return 10**(-SNR_db/10)\n",
    "\n",
    "def kay_weights(N):\n",
    "    scaling = (3.0/2)*N/(N**2 - 1)\n",
    "    \n",
    "    w = [1 - ((i - (N/2 - 1))/(N/2))**2 for i in range(N-1)]\n",
    "    \n",
    "    return scaling*np.array(w)\n",
    "\n",
    "def kays_method(my_signal):\n",
    "    N = len(my_signal)\n",
    "    w = kay_weights(N)\n",
    "    \n",
    "    angle_diff = np.angle(np.conj(my_signal[0:-1])*my_signal[1:])\n",
    "    need_to_shift = np.any(angle_diff < -np.pi/2)\n",
    "    if need_to_shift:    \n",
    "        neg_idx = angle_diff < 0\n",
    "        angle_diff[neg_idx] += np.pi*2\n",
    "    \n",
    "    return w.dot(angle_diff)\n",
    "\n",
    "def kays_singleton_accuracy(test_signals, test_freqs, N):\n",
    "    diffs = [s - make_signal(kays_method(s), 0, N) for s in test_signals]\n",
    "    thresh, single_acc, other_acc, best_thresh = 0.0, 0, 0, 0\n",
    "    best = 0\n",
    "    for i in range(150):\n",
    "        vals = [(sum(np.absolute(s)) / N) < thresh for s in diffs]\n",
    "        corr = [1 for i in range(len(test_freqs)) if (test_freqs[i] == [0, 1, 0] and vals[i] == 1) or ((test_freqs[i] != [0, 1, 0] and vals[i] == 0))]\n",
    "        corr = sum(corr)\n",
    "        #single = sum([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]])\n",
    "        #other = sum([not vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]])        \n",
    "        #if single*2 + other > single_acc*2 + other_acc and single > 0.2 and other > 0.2:\n",
    "        #    single_acc = single\n",
    "        #    other_acc = other\n",
    "        #    best_thresh = thresh\n",
    "        if corr > best:\n",
    "            best = corr\n",
    "            best_thresh = thresh\n",
    "        thresh += 0.05\n",
    "    print('thresh: ', best_thresh)\n",
    "    return best / len(test_signals)\n",
    "\n",
    "def test_kays(signals, freqs, N):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        res = kays_method(sig)\n",
    "        res = round(res * N / (2 * np.pi))\n",
    "        if np.argmax(freq) == res:\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "\n",
    "def test_mle(signals, freqs, N, m):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        cleans = [make_signal(np.pi * 2 * w / N, 0, m) for w in range(N)]\n",
    "        dots = [np.absolute(np.vdot(sig, clean)) for clean in cleans]\n",
    "        if np.argmax(dots) == np.argmax(freq):\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "    \n",
    "def make_binary(freqs, N):\n",
    "    w = math.ceil(np.log2(N))\n",
    "    return [[int(a) for a in list(np.binary_repr(f, width=w))] for f in freqs] \n",
    "\n",
    "def binary_to_int(binary_string):\n",
    "    return tf.reduce_sum(\n",
    "    tf.cast(tf.reverse(tensor=binary_string, axis=[0]), dtype=tf.int64)\n",
    "    * 2 ** tf.range(tf.cast(tf.size(binary_string), dtype=tf.int64)))\n",
    "    '''y = 0\n",
    "    for i,j in enumerate(x):\n",
    "        y += j<<i\n",
    "    return y'''\n",
    "\n",
    "def hamming(pred, act):\n",
    "    return np.count_nonzero(pred != act)\n",
    "\n",
    "def one_hot(N, batch_size, freqs):\n",
    "    freqs_one_hot = np.zeros((batch_size, N))\n",
    "    freqs_one_hot[np.arange(batch_size), freqs] = 1\n",
    "    return freqs_one_hot\n",
    "\n",
    "def test_noisy_mle(N, m, signals, freqs):\n",
    "    count = 0  \n",
    "    '''imag_signals = []\n",
    "    for index in range(len(signals)):\n",
    "        sig = signals[index]\n",
    "        imag_sig = [(sig[i] + 1j*sig[i+1]) for i in np.arange(len(sig), step=2)]\n",
    "        imag_signals.append(imag_sig)'''\n",
    "    cleans = [make_signal(2*np.pi*i/N, 0, m) for i in range(N)]\n",
    "                     \n",
    "    for index in range(len(signals)):\n",
    "        dots = [np.absolute(np.vdot(signals[index], cleans[i])) for i in range(N)]\n",
    "        if np.argmax(freqs[index]) == np.argmax(dots):\n",
    "            #print(np.argmax(dots))\n",
    "            count += 1\n",
    "    return count / len(freqs)\n",
    "\n",
    "def bit_to_freq(bits, N):\n",
    "    possible = [i for i in range(N)]\n",
    "    for b in bits:\n",
    "        if b[0]:\n",
    "            possible = possible[:len(possible)//2]\n",
    "        else:\n",
    "            possible = possible[len(possible)//2:]\n",
    "    return possible[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.442\n",
      "0.381\n",
      "0.325\n",
      "0.258\n",
      "0.206\n",
      "0.162\n",
      "0.112\n",
      "0.117\n"
     ]
    }
   ],
   "source": [
    "# test mle detection for singletons\n",
    "\n",
    "snrs = [8, 6, 4, 2, 0, -2, -4, -6]\n",
    "N = 27000 \n",
    "m = 300\n",
    "batch_size = 1000\n",
    "\n",
    "res = []\n",
    "\n",
    "for SNRdB in snrs:\n",
    "    test_signals, test_freqs = make_batch_noisy(batch_size, SNRdB, N, m, binary=False)\n",
    "    #test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "    #test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "    #test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "    res.append(test_noisy_mle(N, m, test_signals, test_freqs))\n",
    "    print(res[-1])\n",
    "    \n",
    "    \n",
    "np.save('./data/freq_detect/snrs', snrs)\n",
    "np.save('./data/freq_detect/mle_acc', res)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6954, Training Accuracy= 0.488\n",
      "Iter 1000, Minibatch Loss= 0.6917, Training Accuracy= 0.510\n",
      "Iter 1500, Minibatch Loss= 0.6923, Training Accuracy= 0.514\n",
      "Iter 2000, Minibatch Loss= 0.6925, Training Accuracy= 0.513\n",
      "Iter 2500, Minibatch Loss= 0.6927, Training Accuracy= 0.513\n",
      "Iter 3000, Minibatch Loss= 0.6927, Training Accuracy= 0.513\n",
      "Iter 3500, Minibatch Loss= 0.6927, Training Accuracy= 0.513\n",
      "Iter 4000, Minibatch Loss= 0.6928, Training Accuracy= 0.513\n",
      "Iter 4500, Minibatch Loss= 0.6928, Training Accuracy= 0.513\n",
      "Iter 5000, Minibatch Loss= 0.6929, Training Accuracy= 0.513\n",
      "Iter 5500, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 6000, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 6500, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 7000, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 7500, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 8000, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 8500, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 9000, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 9500, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Iter 10000, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Training Finished\n",
      "0.48846152\n"
     ]
    }
   ],
   "source": [
    "# div and conquer freq detect\n",
    "\n",
    "N = 8192 \n",
    "SNRdB = 0\n",
    "m = 50 \n",
    "\n",
    "log = int(np.log2(N))\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "num_iter = 10000\n",
    "batch_size = log * 100\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_classes = 2\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, m, 2])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([5, 2, 2])), # filtersize, in channels, outchannels\n",
    "    'out': tf.Variable(tf.random_normal([(m-4-2-2)*2, num_classes])),\n",
    "    'h2': tf.Variable(tf.random_normal([3, 2, 2])),\n",
    "    'h3': tf.Variable(tf.random_normal([3, 2, 2]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes])),\n",
    "    'b2': tf.Variable(tf.random_normal([2])),\n",
    "    'b3': tf.Variable(tf.random_normal([2]))\n",
    "}\n",
    "\n",
    "\n",
    "test_signals, test_freqs, freqs = make_batch_noisy_lohi(batch_size // log, SNRdB, N, m)\n",
    "test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "\n",
    "\n",
    "training_size = 500\n",
    "dict = {}\n",
    "for i in range(training_size):\n",
    "    batch_x, batch_y, batch_freqs = make_batch_noisy_lohi(batch_size // log, SNRdB, N, m)\n",
    "    batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "    batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "    batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "    dict[i] = (batch_x_pair, batch_y)\n",
    "\n",
    "def neural_net(x):\n",
    "    layer_1 = tf.add(tf.nn.conv1d(x, weights['h1'], 1, 'VALID'), biases['b1'])\n",
    "    hidden_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.nn.conv1d(hidden_1, weights['h2'], 1, 'VALID'), biases['b2'])\n",
    "    hidden_2 = tf.nn.relu(layer_2)\n",
    "    layer_3 = tf.add(tf.nn.conv1d(hidden_2, weights['h3'], 1, 'VALID'), biases['b3'])\n",
    "    hidden_3 = tf.nn.relu(layer_3)\n",
    "    hidden_3 = tf.reshape(hidden_3, [batch_size, -1])\n",
    "    out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "losses, accuracies = [], []\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))  \n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    print(\"Training Started\")\n",
    "\n",
    "    for step in range(1, num_iter + 1):\n",
    "        batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "        if step % 500 == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                 Y: batch_y})\n",
    "\n",
    "            accuracies.append(acc)\n",
    "            losses.append(loss)\n",
    "            print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "    print(\"Training Finished\")\n",
    "    preds, nn_acc = sess.run([prediction, accuracy], feed_dict={X: test_signals_pair, Y: test_freqs})  \n",
    "    print(nn_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "preds = np.round(preds)\n",
    "fs = []\n",
    "for k in range(len(preds) // log):\n",
    "    fs.append(bit_to_freq(preds[k * log : (k+1) * log], N))\n",
    "corr = [fs[i] == freqs[i] % N for i in range(len(fs))]\n",
    "print(sum(corr) / len(fs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
