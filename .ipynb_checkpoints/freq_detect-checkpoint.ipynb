{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signal(w,theta,n):\n",
    "    \"\"\"\n",
    "    Assumes normalized amplitude\n",
    "    \"\"\"\n",
    "    t = np.arange(n)\n",
    "    signal = np.exp(1j*(w*t + theta))\n",
    "    return signal\n",
    "\n",
    "def make_noise(sigma2,n):\n",
    "    noise_scaling = np.sqrt(sigma2/2)\n",
    "    # noise is complex valued\n",
    "    noise  = noise_scaling*np.random.randn(n) + 1j*noise_scaling*np.random.randn(n)\n",
    "    return noise\n",
    "\n",
    "def make_noisy_signal(w,theta,SNRdb,n):\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdb)\n",
    "    signal = make_signal(w,theta,n)\n",
    "    noise  = make_noise(sigma2,n)\n",
    "    return signal + noise\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "def make_batch_noisy(batch_size, SNRdb, N, m, binary=False):\n",
    "    signals, freqs = [], []\n",
    "    for i in range(batch_size):\n",
    "        freq = np.random.randint(0, N)\n",
    "        w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "        sig = make_noisy_signal(w, 0, SNRdb, m)\n",
    "        signals.append(sig)\n",
    "        freqs.append(freq)\n",
    "    if binary:\n",
    "        return signals, make_binary(freqs, N), one_hot(N, batch_size, freqs)\n",
    "    return signals, one_hot(N, batch_size, freqs)\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "# starts = shift for each subsequent sample\n",
    "def make_batch_noisy_lohi(batch_size, SNRdb, N, m, starts=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]):\n",
    "    freqs = []\n",
    "    freqs.append(np.random.randint(0, N))\n",
    "    test_signals, test_freqs = make_noisy_lohi(SNRdB, N, m, freqs[-1], starts)\n",
    "    for i in range(1, batch_size):\n",
    "        freqs.append(np.random.randint(0, N))\n",
    "        a, b = make_noisy_lohi(SNRdB, N, m, freqs[-1], starts)\n",
    "        test_signals.extend(a)\n",
    "        test_freqs.extend(b)\n",
    "    return test_signals, test_freqs, freqs\n",
    "\n",
    "def make_noisy_lohi(SNRdb, N, m, freq, starts):\n",
    "    signals, vals = [], []\n",
    "    steps = int(np.log2(N))\n",
    "    w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "    sig = make_noisy_signal(w, 0, SNRdb, N)\n",
    "    #start = 0\n",
    "    for i in range(int(np.log2(N))):\n",
    "        #start = start + np.random.randint(N // 4) if i > 0 else 0\n",
    "        signals.append([sig[(starts[i] + a * (2**i)) % N] for a in range(m)])\n",
    "        if (freq * (2**i)) % (N) < N / 2:\n",
    "            vals.append([0])\n",
    "        else:\n",
    "            vals.append([1])\n",
    "    return signals, vals\n",
    "        \n",
    "\n",
    "def make_batch_singleton(batch_size, SNRdb, N, m, default=-1): # 0 = zero, 1 = single, 2 = multi\n",
    "    signals, freqs = [], []\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdB)\n",
    "    for i in range(batch_size):\n",
    "        val = np.random.poisson(0.79)\n",
    "        if default >= 0:\n",
    "            val = default\n",
    "        if val == 0:\n",
    "            signals.append(make_noise(0, m))\n",
    "            freqs.append([1, 0, 0])\n",
    "        if val == 1:\n",
    "            signals.append(make_noisy_signal(2 * np.pi * np.random.randint(0, N) / N, 0, SNRdB, m))\n",
    "            freqs.append([0, 1, 0])\n",
    "        if val >= 2:\n",
    "            signal = make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            for i in range(val - 1):\n",
    "                signal += make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            signals.append(signal + make_noise(sigma2, m))\n",
    "            freqs.append([0, 0, 1])\n",
    "    return signals, freqs\n",
    "\n",
    "def get_sigma2_from_snrdb(SNR_db):\n",
    "    return 10**(-SNR_db/10)\n",
    "\n",
    "def kay_weights(N):\n",
    "    scaling = (3.0/2)*N/(N**2 - 1)\n",
    "    \n",
    "    w = [1 - ((i - (N/2 - 1))/(N/2))**2 for i in range(N-1)]\n",
    "    \n",
    "    return scaling*np.array(w)\n",
    "\n",
    "def kays_method(my_signal):\n",
    "    N = len(my_signal)\n",
    "    w = kay_weights(N)\n",
    "    \n",
    "    angle_diff = np.angle(np.conj(my_signal[0:-1])*my_signal[1:])\n",
    "    need_to_shift = np.any(angle_diff < -np.pi/2)\n",
    "    if need_to_shift:    \n",
    "        neg_idx = angle_diff < 0\n",
    "        angle_diff[neg_idx] += np.pi*2\n",
    "    \n",
    "    return w.dot(angle_diff)\n",
    "\n",
    "def kays_singleton_accuracy(test_signals, test_freqs, N):\n",
    "    diffs = [s - make_signal(kays_method(s), 0, N) for s in test_signals]\n",
    "    thresh, single_acc, other_acc, best_thresh = 0.0, 0, 0, 0\n",
    "    best = 0\n",
    "    for i in range(150):\n",
    "        vals = [(sum(np.absolute(s)) / N) < thresh for s in diffs]\n",
    "        corr = [1 for i in range(len(test_freqs)) if (test_freqs[i] == [0, 1, 0] and vals[i] == 1) or ((test_freqs[i] != [0, 1, 0] and vals[i] == 0))]\n",
    "        corr = sum(corr)\n",
    "        #single = sum([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]])\n",
    "        #other = sum([not vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]])        \n",
    "        #if single*2 + other > single_acc*2 + other_acc and single > 0.2 and other > 0.2:\n",
    "        #    single_acc = single\n",
    "        #    other_acc = other\n",
    "        #    best_thresh = thresh\n",
    "        if corr > best:\n",
    "            best = corr\n",
    "            best_thresh = thresh\n",
    "        thresh += 0.05\n",
    "    print('thresh: ', best_thresh)\n",
    "    return best / len(test_signals)\n",
    "\n",
    "def test_kays(signals, freqs, N):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        res = kays_method(sig)\n",
    "        res = round(res * N / (2 * np.pi))\n",
    "        if np.argmax(freq) == res:\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "\n",
    "def test_mle(signals, freqs, N, m):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        cleans = [make_signal(np.pi * 2 * w / N, 0, m) for w in range(N)]\n",
    "        dots = [np.absolute(np.vdot(sig, clean)) for clean in cleans]\n",
    "        if np.argmax(dots) == np.argmax(freq):\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "    \n",
    "def make_binary(freqs, N):\n",
    "    w = math.ceil(np.log2(N))\n",
    "    return [[int(a) for a in list(np.binary_repr(f, width=w))] for f in freqs] \n",
    "\n",
    "def binary_to_int(binary_string):\n",
    "    return tf.reduce_sum(\n",
    "    tf.cast(tf.reverse(tensor=binary_string, axis=[0]), dtype=tf.int64)\n",
    "    * 2 ** tf.range(tf.cast(tf.size(binary_string), dtype=tf.int64)))\n",
    "    '''y = 0\n",
    "    for i,j in enumerate(x):\n",
    "        y += j<<i\n",
    "    return y'''\n",
    "\n",
    "def hamming(pred, act):\n",
    "    return np.count_nonzero(pred != act)\n",
    "\n",
    "def one_hot(N, batch_size, freqs):\n",
    "    freqs_one_hot = np.zeros((batch_size, N))\n",
    "    freqs_one_hot[np.arange(batch_size), freqs] = 1\n",
    "    return freqs_one_hot\n",
    "\n",
    "def test_noisy_mle(N, m, signals, freqs):\n",
    "    count = 0  \n",
    "    '''imag_signals = []\n",
    "    for index in range(len(signals)):\n",
    "        sig = signals[index]\n",
    "        imag_sig = [(sig[i] + 1j*sig[i+1]) for i in np.arange(len(sig), step=2)]\n",
    "        imag_signals.append(imag_sig)'''\n",
    "    cleans = [make_signal(2*np.pi*i/N, 0, m) for i in range(N)]\n",
    "                     \n",
    "    for index in range(len(signals)):\n",
    "        dots = [np.absolute(np.vdot(signals[index], cleans[i])) for i in range(N)]\n",
    "        if np.argmax(freqs[index]) == np.argmax(dots):\n",
    "            #print(np.argmax(dots))\n",
    "            count += 1\n",
    "    return count / len(freqs)\n",
    "\n",
    "def bit_to_freq(bits, N):\n",
    "    possible = [i for i in range(N)]\n",
    "    for b in bits:\n",
    "        if not b[0]:\n",
    "            possible = possible[:len(possible)//2]\n",
    "        else:\n",
    "            possible = possible[len(possible)//2:]\n",
    "    return possible[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.5454, Training Accuracy= 0.745\n",
      "Iter 1000, Minibatch Loss= 0.1077, Training Accuracy= 0.975\n",
      "Iter 1500, Minibatch Loss= 0.0443, Training Accuracy= 0.990\n",
      "Iter 2000, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Iter 2500, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "Iter 3000, Minibatch Loss= 0.0172, Training Accuracy= 0.995\n",
      "Iter 3500, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "Iter 4000, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "Iter 4500, Minibatch Loss= 0.0032, Training Accuracy= 1.000\n",
      "Iter 5000, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "Iter 5500, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "Iter 6000, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "Iter 6500, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "Iter 7000, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Iter 7500, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "Iter 8000, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Iter 8500, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "Iter 9000, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Iter 9500, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Iter 10000, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Training Finished\n",
      "[0.995, 1.0, 1.0, 1.0, 0.995, 0.985, 1.0, 1.0, 1.0, 0.995]\n",
      "1\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.0914, Training Accuracy= 0.980\n",
      "Iter 1000, Minibatch Loss= 0.0526, Training Accuracy= 0.990\n",
      "Iter 1500, Minibatch Loss= 0.0476, Training Accuracy= 0.985\n",
      "Iter 2000, Minibatch Loss= 0.0451, Training Accuracy= 0.985\n",
      "Iter 2500, Minibatch Loss= 0.0391, Training Accuracy= 0.990\n",
      "Iter 3000, Minibatch Loss= 0.0205, Training Accuracy= 0.990\n",
      "Iter 3500, Minibatch Loss= 0.0191, Training Accuracy= 0.990\n",
      "Iter 4000, Minibatch Loss= 0.0212, Training Accuracy= 0.990\n",
      "Iter 4500, Minibatch Loss= 0.0204, Training Accuracy= 0.990\n",
      "Iter 5000, Minibatch Loss= 0.0196, Training Accuracy= 0.990\n",
      "Iter 5500, Minibatch Loss= 0.0174, Training Accuracy= 0.990\n",
      "Iter 6000, Minibatch Loss= 0.0167, Training Accuracy= 0.990\n",
      "Iter 6500, Minibatch Loss= 0.0166, Training Accuracy= 0.990\n",
      "Iter 7000, Minibatch Loss= 0.0140, Training Accuracy= 0.995\n",
      "Iter 7500, Minibatch Loss= 0.0139, Training Accuracy= 0.995\n",
      "Iter 8000, Minibatch Loss= 0.0141, Training Accuracy= 0.995\n",
      "Iter 8500, Minibatch Loss= 0.0151, Training Accuracy= 0.995\n",
      "Iter 9000, Minibatch Loss= 0.0130, Training Accuracy= 0.995\n",
      "Iter 9500, Minibatch Loss= 0.0130, Training Accuracy= 0.995\n",
      "Iter 10000, Minibatch Loss= 0.0132, Training Accuracy= 0.995\n",
      "Training Finished\n",
      "[0.995, 0.975, 0.99, 1.0, 0.965, 0.985, 1.0, 0.98, 0.975, 0.985]\n",
      "2\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.4626, Training Accuracy= 0.735\n",
      "Iter 1000, Minibatch Loss= 0.1065, Training Accuracy= 0.965\n",
      "Iter 1500, Minibatch Loss= 0.0271, Training Accuracy= 0.990\n",
      "Iter 2000, Minibatch Loss= 0.0166, Training Accuracy= 0.995\n",
      "Iter 2500, Minibatch Loss= 0.0171, Training Accuracy= 0.995\n",
      "Iter 3000, Minibatch Loss= 0.0170, Training Accuracy= 0.995\n",
      "Iter 3500, Minibatch Loss= 0.0156, Training Accuracy= 0.995\n",
      "Iter 4000, Minibatch Loss= 0.0249, Training Accuracy= 0.990\n",
      "Iter 4500, Minibatch Loss= 0.0174, Training Accuracy= 0.995\n",
      "Iter 5000, Minibatch Loss= 0.0132, Training Accuracy= 0.995\n",
      "Iter 5500, Minibatch Loss= 0.0171, Training Accuracy= 0.995\n",
      "Iter 6000, Minibatch Loss= 0.0090, Training Accuracy= 0.995\n",
      "Iter 6500, Minibatch Loss= 0.0115, Training Accuracy= 0.995\n",
      "Iter 7000, Minibatch Loss= 0.0143, Training Accuracy= 0.995\n",
      "Iter 7500, Minibatch Loss= 0.0081, Training Accuracy= 0.995\n",
      "Iter 8000, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "Iter 8500, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "Iter 9000, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "Iter 9500, Minibatch Loss= 0.0074, Training Accuracy= 0.995\n",
      "Iter 10000, Minibatch Loss= 0.0088, Training Accuracy= 0.995\n",
      "Training Finished\n",
      "[1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.99, 0.99, 0.995, 1.0]\n",
      "3\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6951, Training Accuracy= 0.445\n",
      "Iter 1000, Minibatch Loss= 0.6941, Training Accuracy= 0.445\n",
      "Iter 1500, Minibatch Loss= 0.6933, Training Accuracy= 0.445\n",
      "Iter 2000, Minibatch Loss= 0.6928, Training Accuracy= 0.555\n",
      "Iter 2500, Minibatch Loss= 0.6930, Training Accuracy= 0.555\n",
      "Iter 3000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 3500, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 4000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 4500, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 5000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 5500, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 6000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 6500, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 7000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 7500, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 8000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 8500, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 9000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 9500, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Iter 10000, Minibatch Loss= 0.6935, Training Accuracy= 0.445\n",
      "Training Finished\n",
      "[0.515, 0.45, 0.51, 0.48, 0.55, 0.49, 0.485, 0.535, 0.505, 0.47]\n",
      "1.0\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.7360, Training Accuracy= 0.475\n",
      "Iter 1000, Minibatch Loss= 0.7016, Training Accuracy= 0.465\n",
      "Iter 1500, Minibatch Loss= 0.6991, Training Accuracy= 0.450\n",
      "Iter 2000, Minibatch Loss= 0.7004, Training Accuracy= 0.450\n",
      "Iter 2500, Minibatch Loss= 0.6996, Training Accuracy= 0.450\n",
      "Iter 3000, Minibatch Loss= 0.6985, Training Accuracy= 0.455\n",
      "Iter 3500, Minibatch Loss= 0.6918, Training Accuracy= 0.485\n",
      "Iter 4000, Minibatch Loss= 0.6854, Training Accuracy= 0.490\n",
      "Iter 4500, Minibatch Loss= 0.6837, Training Accuracy= 0.505\n",
      "Iter 5000, Minibatch Loss= 0.6950, Training Accuracy= 0.540\n",
      "Iter 5500, Minibatch Loss= 0.6337, Training Accuracy= 0.620\n",
      "Iter 6000, Minibatch Loss= 0.5945, Training Accuracy= 0.680\n",
      "Iter 6500, Minibatch Loss= 0.5546, Training Accuracy= 0.700\n",
      "Iter 7000, Minibatch Loss= 0.5148, Training Accuracy= 0.750\n",
      "Iter 7500, Minibatch Loss= 0.4762, Training Accuracy= 0.780\n",
      "Iter 8000, Minibatch Loss= 0.4185, Training Accuracy= 0.810\n",
      "Iter 8500, Minibatch Loss= 0.3809, Training Accuracy= 0.835\n",
      "Iter 9000, Minibatch Loss= 0.3473, Training Accuracy= 0.830\n",
      "Iter 9500, Minibatch Loss= 0.3131, Training Accuracy= 0.870\n",
      "Iter 10000, Minibatch Loss= 0.2844, Training Accuracy= 0.890\n",
      "Training Finished\n",
      "[0.925, 0.91, 0.895, 0.88, 0.92, 0.92, 0.885, 0.86, 0.95, 0.93]\n",
      "1\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6543, Training Accuracy= 0.655\n",
      "Iter 1000, Minibatch Loss= 0.6223, Training Accuracy= 0.645\n",
      "Iter 1500, Minibatch Loss= 0.5653, Training Accuracy= 0.720\n",
      "Iter 2000, Minibatch Loss= 0.4945, Training Accuracy= 0.760\n",
      "Iter 2500, Minibatch Loss= 0.4427, Training Accuracy= 0.780\n",
      "Iter 3000, Minibatch Loss= 0.4258, Training Accuracy= 0.795\n",
      "Iter 3500, Minibatch Loss= 0.3942, Training Accuracy= 0.820\n",
      "Iter 4000, Minibatch Loss= 0.3949, Training Accuracy= 0.805\n",
      "Iter 4500, Minibatch Loss= 0.3989, Training Accuracy= 0.820\n",
      "Iter 5000, Minibatch Loss= 0.3992, Training Accuracy= 0.810\n",
      "Iter 5500, Minibatch Loss= 0.3982, Training Accuracy= 0.825\n",
      "Iter 6000, Minibatch Loss= 0.4086, Training Accuracy= 0.825\n",
      "Iter 6500, Minibatch Loss= 0.3981, Training Accuracy= 0.845\n",
      "Iter 7000, Minibatch Loss= 0.3237, Training Accuracy= 0.865\n",
      "Iter 7500, Minibatch Loss= 0.1788, Training Accuracy= 0.945\n",
      "Iter 8000, Minibatch Loss= 0.1433, Training Accuracy= 0.955\n",
      "Iter 8500, Minibatch Loss= 0.1415, Training Accuracy= 0.955\n",
      "Iter 9000, Minibatch Loss= 0.1239, Training Accuracy= 0.980\n",
      "Iter 9500, Minibatch Loss= 0.1107, Training Accuracy= 0.980\n",
      "Iter 10000, Minibatch Loss= 0.1086, Training Accuracy= 0.975\n",
      "Training Finished\n",
      "[0.905, 0.92, 0.895, 0.965, 0.92, 0.925, 0.925, 0.91, 0.855, 0.915]\n",
      "2\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6822, Training Accuracy= 0.570\n",
      "Iter 1000, Minibatch Loss= 0.5151, Training Accuracy= 0.795\n",
      "Iter 1500, Minibatch Loss= 0.2243, Training Accuracy= 0.925\n",
      "Iter 2000, Minibatch Loss= 0.1717, Training Accuracy= 0.945\n",
      "Iter 2500, Minibatch Loss= 0.1677, Training Accuracy= 0.945\n",
      "Iter 3000, Minibatch Loss= 0.1641, Training Accuracy= 0.945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3500, Minibatch Loss= 0.1645, Training Accuracy= 0.945\n",
      "Iter 4000, Minibatch Loss= 0.1607, Training Accuracy= 0.950\n",
      "Iter 4500, Minibatch Loss= 0.1566, Training Accuracy= 0.955\n",
      "Iter 5000, Minibatch Loss= 0.1549, Training Accuracy= 0.935\n",
      "Iter 5500, Minibatch Loss= 0.1531, Training Accuracy= 0.935\n",
      "Iter 6000, Minibatch Loss= 0.1496, Training Accuracy= 0.945\n",
      "Iter 6500, Minibatch Loss= 0.1449, Training Accuracy= 0.950\n",
      "Iter 7000, Minibatch Loss= 0.1454, Training Accuracy= 0.955\n",
      "Iter 7500, Minibatch Loss= 0.1437, Training Accuracy= 0.950\n",
      "Iter 8000, Minibatch Loss= 0.1459, Training Accuracy= 0.950\n",
      "Iter 8500, Minibatch Loss= 0.1443, Training Accuracy= 0.955\n",
      "Iter 9000, Minibatch Loss= 0.1437, Training Accuracy= 0.960\n",
      "Iter 9500, Minibatch Loss= 0.1410, Training Accuracy= 0.960\n",
      "Iter 10000, Minibatch Loss= 0.1424, Training Accuracy= 0.950\n",
      "Training Finished\n",
      "[0.95, 0.94, 0.925, 0.945, 0.935, 0.945, 0.93, 0.96, 0.915, 0.91]\n",
      "3\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6633, Training Accuracy= 0.580\n",
      "Iter 1000, Minibatch Loss= 0.5693, Training Accuracy= 0.705\n",
      "Iter 1500, Minibatch Loss= 0.4950, Training Accuracy= 0.775\n",
      "Iter 2000, Minibatch Loss= 0.3945, Training Accuracy= 0.830\n",
      "Iter 2500, Minibatch Loss= 0.3698, Training Accuracy= 0.850\n",
      "Iter 3000, Minibatch Loss= 0.3652, Training Accuracy= 0.855\n",
      "Iter 3500, Minibatch Loss= 0.3741, Training Accuracy= 0.855\n",
      "Iter 4000, Minibatch Loss= 0.3694, Training Accuracy= 0.865\n",
      "Iter 4500, Minibatch Loss= 0.3557, Training Accuracy= 0.870\n",
      "Iter 5000, Minibatch Loss= 0.3547, Training Accuracy= 0.865\n",
      "Iter 5500, Minibatch Loss= 0.3531, Training Accuracy= 0.870\n",
      "Iter 6000, Minibatch Loss= 0.3470, Training Accuracy= 0.880\n",
      "Iter 6500, Minibatch Loss= 0.3442, Training Accuracy= 0.880\n",
      "Iter 7000, Minibatch Loss= 0.3426, Training Accuracy= 0.875\n",
      "Iter 7500, Minibatch Loss= 0.3450, Training Accuracy= 0.870\n",
      "Iter 8000, Minibatch Loss= 0.3439, Training Accuracy= 0.865\n",
      "Iter 8500, Minibatch Loss= 0.3410, Training Accuracy= 0.860\n",
      "Iter 9000, Minibatch Loss= 0.3409, Training Accuracy= 0.855\n",
      "Iter 9500, Minibatch Loss= 0.3405, Training Accuracy= 0.860\n",
      "Iter 10000, Minibatch Loss= 0.3357, Training Accuracy= 0.865\n",
      "Training Finished\n",
      "[0.86, 0.9, 0.895, 0.925, 0.905, 0.91, 0.855, 0.92, 0.865, 0.88]\n",
      "0.9375\n",
      "0.56\n",
      "-2\n",
      "0\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6910, Training Accuracy= 0.540\n",
      "Iter 1000, Minibatch Loss= 0.6942, Training Accuracy= 0.470\n",
      "Iter 1500, Minibatch Loss= 0.6931, Training Accuracy= 0.480\n",
      "Iter 2000, Minibatch Loss= 0.6928, Training Accuracy= 0.470\n",
      "Iter 2500, Minibatch Loss= 0.6889, Training Accuracy= 0.480\n",
      "Iter 3000, Minibatch Loss= 0.6809, Training Accuracy= 0.510\n",
      "Iter 3500, Minibatch Loss= 0.6440, Training Accuracy= 0.610\n",
      "Iter 4000, Minibatch Loss= 0.5741, Training Accuracy= 0.715\n",
      "Iter 4500, Minibatch Loss= 0.5005, Training Accuracy= 0.765\n",
      "Iter 5000, Minibatch Loss= 0.4861, Training Accuracy= 0.780\n",
      "Iter 5500, Minibatch Loss= 0.4668, Training Accuracy= 0.800\n",
      "Iter 6000, Minibatch Loss= 0.4505, Training Accuracy= 0.790\n",
      "Iter 6500, Minibatch Loss= 0.4453, Training Accuracy= 0.805\n",
      "Iter 7000, Minibatch Loss= 0.4381, Training Accuracy= 0.810\n",
      "Iter 7500, Minibatch Loss= 0.4379, Training Accuracy= 0.815\n",
      "Iter 8000, Minibatch Loss= 0.4375, Training Accuracy= 0.825\n",
      "Iter 8500, Minibatch Loss= 0.4344, Training Accuracy= 0.820\n",
      "Iter 9000, Minibatch Loss= 0.4363, Training Accuracy= 0.820\n",
      "Iter 9500, Minibatch Loss= 0.4342, Training Accuracy= 0.810\n",
      "Iter 10000, Minibatch Loss= 0.4263, Training Accuracy= 0.805\n",
      "Training Finished\n",
      "[0.825, 0.81, 0.83, 0.82, 0.835, 0.775, 0.77, 0.85, 0.815, 0.77]\n",
      "1\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6963, Training Accuracy= 0.520\n",
      "Iter 1000, Minibatch Loss= 0.6965, Training Accuracy= 0.535\n",
      "Iter 1500, Minibatch Loss= 0.6968, Training Accuracy= 0.525\n",
      "Iter 2000, Minibatch Loss= 0.6965, Training Accuracy= 0.535\n",
      "Iter 2500, Minibatch Loss= 0.6969, Training Accuracy= 0.555\n",
      "Iter 3000, Minibatch Loss= 0.6652, Training Accuracy= 0.630\n",
      "Iter 3500, Minibatch Loss= 0.5484, Training Accuracy= 0.745\n",
      "Iter 4000, Minibatch Loss= 0.4699, Training Accuracy= 0.785\n",
      "Iter 4500, Minibatch Loss= 0.4168, Training Accuracy= 0.820\n",
      "Iter 5000, Minibatch Loss= 0.3874, Training Accuracy= 0.865\n",
      "Iter 5500, Minibatch Loss= 0.3463, Training Accuracy= 0.885\n",
      "Iter 6000, Minibatch Loss= 0.3078, Training Accuracy= 0.905\n",
      "Iter 6500, Minibatch Loss= 0.3030, Training Accuracy= 0.905\n",
      "Iter 7000, Minibatch Loss= 0.2826, Training Accuracy= 0.925\n",
      "Iter 7500, Minibatch Loss= 0.2785, Training Accuracy= 0.925\n",
      "Iter 8000, Minibatch Loss= 0.2744, Training Accuracy= 0.940\n",
      "Iter 8500, Minibatch Loss= 0.2730, Training Accuracy= 0.935\n",
      "Iter 9000, Minibatch Loss= 0.2723, Training Accuracy= 0.935\n",
      "Iter 9500, Minibatch Loss= 0.2709, Training Accuracy= 0.935\n",
      "Iter 10000, Minibatch Loss= 0.2701, Training Accuracy= 0.940\n",
      "Training Finished\n",
      "[0.915, 0.92, 0.885, 0.89, 0.935, 0.935, 0.96, 0.905, 0.94, 0.88]\n",
      "2\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6962, Training Accuracy= 0.470\n",
      "Iter 1000, Minibatch Loss= 0.6946, Training Accuracy= 0.470\n",
      "Iter 1500, Minibatch Loss= 0.6933, Training Accuracy= 0.470\n",
      "Iter 2000, Minibatch Loss= 0.6921, Training Accuracy= 0.530\n",
      "Iter 2500, Minibatch Loss= 0.6901, Training Accuracy= 0.530\n",
      "Iter 3000, Minibatch Loss= 0.5818, Training Accuracy= 0.695\n",
      "Iter 3500, Minibatch Loss= 0.5659, Training Accuracy= 0.705\n",
      "Iter 4000, Minibatch Loss= 0.5406, Training Accuracy= 0.730\n",
      "Iter 4500, Minibatch Loss= 0.5394, Training Accuracy= 0.750\n",
      "Iter 5000, Minibatch Loss= 0.3733, Training Accuracy= 0.870\n",
      "Iter 5500, Minibatch Loss= 0.3481, Training Accuracy= 0.885\n",
      "Iter 6000, Minibatch Loss= 0.3316, Training Accuracy= 0.890\n",
      "Iter 6500, Minibatch Loss= 0.3208, Training Accuracy= 0.895\n",
      "Iter 7000, Minibatch Loss= 0.3180, Training Accuracy= 0.895\n",
      "Iter 7500, Minibatch Loss= 0.3168, Training Accuracy= 0.895\n",
      "Iter 8000, Minibatch Loss= 0.3181, Training Accuracy= 0.890\n",
      "Iter 8500, Minibatch Loss= 0.3172, Training Accuracy= 0.900\n",
      "Iter 9000, Minibatch Loss= 0.3160, Training Accuracy= 0.895\n",
      "Iter 9500, Minibatch Loss= 0.3141, Training Accuracy= 0.900\n",
      "Iter 10000, Minibatch Loss= 0.3133, Training Accuracy= 0.900\n",
      "Training Finished\n",
      "[0.895, 0.965, 0.935, 0.94, 0.895, 0.92, 0.935, 0.875, 0.88, 0.92]\n",
      "3\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.7870, Training Accuracy= 0.490\n",
      "Iter 1000, Minibatch Loss= 0.6959, Training Accuracy= 0.535\n",
      "Iter 1500, Minibatch Loss= 0.6884, Training Accuracy= 0.530\n",
      "Iter 2000, Minibatch Loss= 0.6855, Training Accuracy= 0.540\n",
      "Iter 2500, Minibatch Loss= 0.6851, Training Accuracy= 0.540\n",
      "Iter 3000, Minibatch Loss= 0.6812, Training Accuracy= 0.560\n",
      "Iter 3500, Minibatch Loss= 0.6752, Training Accuracy= 0.625\n",
      "Iter 4000, Minibatch Loss= 0.6461, Training Accuracy= 0.605\n",
      "Iter 4500, Minibatch Loss= 0.6285, Training Accuracy= 0.660\n",
      "Iter 5000, Minibatch Loss= 0.6242, Training Accuracy= 0.665\n",
      "Iter 5500, Minibatch Loss= 0.5876, Training Accuracy= 0.670\n",
      "Iter 6000, Minibatch Loss= 0.5078, Training Accuracy= 0.725\n",
      "Iter 6500, Minibatch Loss= 0.4307, Training Accuracy= 0.800\n",
      "Iter 7000, Minibatch Loss= 0.3433, Training Accuracy= 0.845\n",
      "Iter 7500, Minibatch Loss= 0.2519, Training Accuracy= 0.895\n",
      "Iter 8000, Minibatch Loss= 0.2174, Training Accuracy= 0.915\n",
      "Iter 8500, Minibatch Loss= 0.2261, Training Accuracy= 0.915\n",
      "Iter 9000, Minibatch Loss= 0.2258, Training Accuracy= 0.925\n",
      "Iter 9500, Minibatch Loss= 0.2341, Training Accuracy= 0.915\n",
      "Iter 10000, Minibatch Loss= 0.2281, Training Accuracy= 0.905\n",
      "Training Finished\n",
      "[0.92, 0.9, 0.915, 0.905, 0.905, 0.915, 0.915, 0.89, 0.925, 0.885]\n",
      "0.92\n",
      "0.495\n"
     ]
    }
   ],
   "source": [
    "# div and conquer freq detect\n",
    "\n",
    "N = 1024 \n",
    "#SNRdB = -2\n",
    "layer = 6\n",
    "m = 30\n",
    "\n",
    "log = int(np.log2(N))\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "num_iter = 80000\n",
    "batch_size = log * 20\n",
    "\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_classes = 1\n",
    "\n",
    "snrs = [10, 8, 6, 4, 2, 0, -2]\n",
    "\n",
    "\n",
    "bin_accs = []\n",
    "freq_accs = []\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "\n",
    "#starts = [0]\n",
    "#for i in range(1, log): # filling all the starting points\n",
    "#    starts.append(starts[i-1] + np.random.randint(N // 4))\n",
    "\n",
    "starts = [0] * 15\n",
    "\n",
    "for SNRdB in snrs:\n",
    "    print(SNRdB)\n",
    "\n",
    "    t_bins = []\n",
    "    t_freqs = []\n",
    "    t_preds = []\n",
    "    t_actuals = []\n",
    "    \n",
    "    training_size = 5999\n",
    "    dict = {}\n",
    "    for i in range(training_size):\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "        batch_x, batch_y, batch_freqs = make_batch_noisy_lohi(batch_size // log, SNRdB - (i % 3), N, m, starts)\n",
    "        batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "        batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "        batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "        dict[i] = (batch_x_pair, batch_y)\n",
    "    \n",
    "    for trial in range(4):\n",
    "        print(trial)\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, m, 2])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        # Store layers weight & bias\n",
    "        weights = {i: tf.Variable(tf.random_normal([3, 2, 2])) for i in range(1, layer+1)} # increase out channels, less layers\n",
    "        weights[0] = tf.Variable(tf.random_normal([5, 2, 2]))\n",
    "        weights['out'] = tf.Variable(tf.random_normal([(m-4-(0*layer))*2, num_classes]))\n",
    "        biases = {i: tf.Variable(tf.random_normal([2])) for i in range(layer+1)}\n",
    "        biases['out'] = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights[0], 1, 'VALID'), biases[0])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            for i in range(1, layer+1):\n",
    "                layer_1 = tf.add(tf.nn.conv1d(hidden_1, weights[i], 1, 'SAME'), biases[i]) # no padding\n",
    "                hidden_1 = tf.nn.relu(layer_1) # try: elu, leaky\n",
    "                ### instance normalize\n",
    "            hidden_3 = tf.reshape(hidden_1, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "\n",
    "\n",
    "        test_dict = {}\n",
    "        for i in range(10):\n",
    "            test_signals, test_freqs, freqs = make_batch_noisy_lohi(batch_size // log, SNRdB, N, m, starts)\n",
    "            test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "            test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "            test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "            test_dict[i] = (test_signals_pair, test_freqs, freqs)\n",
    "\n",
    "\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net(X)\n",
    "        prediction = tf.nn.sigmoid(logits)\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))  \n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "        # Evaluate model\n",
    "        pred_class = tf.greater(prediction, 0.5)\n",
    "        correct_pred = tf.equal(pred_class, tf.equal(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "            print(\"Training Started\")\n",
    "\n",
    "            for step in range(1, num_iter + 1):\n",
    "                batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "                if step % 500 == 0:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "\n",
    "                    accuracies.append(acc)\n",
    "                    losses.append(loss)\n",
    "                    print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "            print(\"Training Finished\")\n",
    "            preds = [sess.run(prediction, feed_dict={X: test_dict[i][0], Y: test_dict[i][1]}) for i in range(len(test_dict))]  \n",
    "            nn_acc = [sess.run(accuracy, feed_dict={X: test_dict[i][0], Y: test_dict[i][1]}) for i in range(len(test_dict))]   \n",
    "            print(nn_acc)\n",
    "        t_preds.append(preds)\n",
    "        t_actuals.append([test_dict[i][1] for i in range(len(test_dict))])\n",
    "\n",
    "        t_bins.append(np.median(nn_acc))\n",
    "        preds = np.round(preds)\n",
    "        corr = []\n",
    "        for a in range(len(test_dict)):\n",
    "            fs = []\n",
    "            for k in range(len(preds[a]) // log):\n",
    "                    fs.append(bit_to_freq(preds[a][k * log : (k+1) * log], N))\n",
    "            corr.extend([fs[i] == test_dict[a][2][i] % N for i in range(len(fs))])\n",
    "        t_freqs.append(np.sum(corr) / (len(fs) * len(test_dict)))\n",
    "    bin_accs.append(max(t_bins))\n",
    "    freq_accs.append(max(t_freqs))\n",
    "    all_preds.append(t_preds[np.argmax(t_freqs)])\n",
    "    all_actuals.append(t_actuals[np.argmax(t_freqs)])\n",
    "    print(bin_accs[-1])\n",
    "    print(freq_accs[-1])\n",
    "    \n",
    "np.save('./data/testing/snrs', snrs)   \n",
    "np.save('./data/testing/acc_binary', bin_accs)\n",
    "np.save('./data/testing/acc_frequency', freq_accs)\n",
    "np.save('./data/testing/preds_frequency', all_preds)\n",
    "np.save('./data/testing/actuals_frequency', all_actuals)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34\n"
     ]
    }
   ],
   "source": [
    "preds = np.round(preds)\n",
    "corr = []\n",
    "for a in range(len(test_dict)):\n",
    "    fs = []\n",
    "    for k in range(len(preds[a]) // log):\n",
    "            fs.append(bit_to_freq(preds[a][k * log : (k+1) * log], N))\n",
    "    corr.extend([fs[i] == test_dict[a][2][i] % N for i in range(len(fs))])\n",
    "print(np.sum(corr) / (len(fs) * len(test_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303,)\n",
      "499\n",
      "[1 0 0 0 0 0 1 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.load('./data/testing/acc_binary.npy')\n",
    "freqs = np.load('./data/testing/acc_frequency.npy')\n",
    "all_preds = np.load('./data/testing/preds_frequency.npy')\n",
    "all_actuals = np.load('./data/testing/actuals_frequency.npy')\n",
    "preds = np.reshape(all_preds, -1)\n",
    "actuals = np.reshape(all_actuals, -1)\n",
    "wrongs = []\n",
    "for p, a in zip(preds, actuals):\n",
    "    if int(np.round(p)) != int(a):\n",
    "        wrongs.append(p)\n",
    "print(np.shape(wrongs))\n",
    "print(bit_to_freq(np.expand_dims(actuals[0:10], 1), 1024))\n",
    "print(actuals[0:10])\n",
    "plt.hist(wrongs)\n",
    "plt.show()\n",
    "hammings = []\n",
    "for i in range(len(preds) // 10):\n",
    "    hammings.append(np.sum([int(round(preds[i*10 + j])) != int(actuals[i*10 + j]) for j in range(10)]))\n",
    "plt.hist(hammings)\n",
    "plt.show()\n",
    "# bit to freq is just inverse of binary representation for freq - use to calc hamming and confidence vs pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
