{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"lines.dashed_pattern\" on line 18 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"lines.dashdot_pattern\" on line 19 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"lines.dotted_pattern\" on line 20 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"lines.scale_dashes\" on line 21 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"patch.force_edgecolor\" on line 33 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"hatch.color\" on line 37 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"hatch.linewidth\" on line 38 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"hist.bins\" on line 40 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"axes.titlepad\" on line 177 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"axes.formatter.offset_threshold\" on line 200 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"axes.autolimit_mode\" on line 213 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"date.autoformatter.year\" on line 223 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"date.autoformatter.month\" on line 224 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"date.autoformatter.day\" on line 225 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"date.autoformatter.hour\" on line 226 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"date.autoformatter.minute\" on line 227 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"date.autoformatter.second\" on line 228 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"date.autoformatter.microsecond\" on line 229 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"xtick.top\" on line 234 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"xtick.bottom\" on line 235 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"xtick.major.top\" on line 246 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"xtick.major.bottom\" on line 247 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"xtick.minor.top\" on line 248 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"xtick.minor.bottom\" on line 249 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"ytick.left\" on line 251 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"ytick.right\" on line 252 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"ytick.major.left\" on line 263 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"ytick.major.right\" on line 264 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"ytick.minor.left\" on line 265 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"ytick.minor.right\" on line 266 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"scatter.marker\" on line 338 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\__init__.py:1076: UserWarning: Bad val \"auto\" on line #353\n",
      "\t\"boxplot.flierprops.markerfacecolor: auto\n",
      "\"\n",
      "\tin file \"c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle\"\n",
      "\tKey boxplot.flierprops.markerfacecolor: auto does not look like a color arg\n",
      "  (val, error_details, msg))\n",
      "\n",
      "Bad key \"boxplot.meanprops.marker\" on line 360 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"boxplot.meanprops.markerfacecolor\" on line 361 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"boxplot.meanprops.markeredgecolor\" on line 362 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"boxplot.meanprops.markersize\" on line 363 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"_internal.classic_mode\" on line 513 in\n",
      "c:\\users\\jains\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signal(w,theta,n):\n",
    "    \"\"\"\n",
    "    Assumes normalized amplitude\n",
    "    \"\"\"\n",
    "    t = np.arange(n)\n",
    "    signal = np.exp(1j*(w*t + theta))\n",
    "    return signal\n",
    "\n",
    "def make_noise(sigma2,n):\n",
    "    noise_scaling = np.sqrt(sigma2/2)\n",
    "    # noise is complex valued\n",
    "    noise  = noise_scaling*np.random.randn(n) + 1j*noise_scaling*np.random.randn(n)\n",
    "    return noise\n",
    "\n",
    "def make_noisy_signal(w,theta,SNRdb,n):\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdb)\n",
    "    signal = make_signal(w,theta,n)\n",
    "    noise  = make_noise(sigma2,n)\n",
    "    return signal + noise\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "def make_batch_noisy(batch_size, SNRdb, N, m, binary=False):\n",
    "    signals, freqs = [], []\n",
    "    for i in range(batch_size):\n",
    "        freq = np.random.randint(0, N)\n",
    "        w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "        sig = make_noisy_signal(w, 0, SNRdb, m)\n",
    "        signals.append(sig)\n",
    "        freqs.append(freq)\n",
    "    if binary:\n",
    "        return signals, make_binary(freqs, N), one_hot(N, batch_size, freqs)\n",
    "    return signals, one_hot(N, batch_size, freqs)\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "# starts = shift for each subsequent sample\n",
    "def make_batch_noisy_lohi(batch_size, SNRdb, N, m, starts=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]):\n",
    "    freqs = []\n",
    "    freqs.append(np.random.randint(0, N))\n",
    "    test_signals, test_freqs = make_noisy_lohi(SNRdB, N, m, freqs[-1], starts)\n",
    "    for i in range(1, batch_size):\n",
    "        freqs.append(np.random.randint(0, N))\n",
    "        a, b = make_noisy_lohi(SNRdB, N, m, freqs[-1], starts)\n",
    "        test_signals.extend(a)\n",
    "        test_freqs.extend(b)\n",
    "    return test_signals, test_freqs, freqs\n",
    "\n",
    "def make_noisy_lohi(SNRdb, N, m, freq, starts):\n",
    "    signals, vals = [], []\n",
    "    steps = int(np.log2(N))\n",
    "    w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "    sig = make_noisy_signal(w, 0, SNRdb, N)\n",
    "    #start = 0\n",
    "    for i in range(int(np.log2(N))):\n",
    "        #start = start + np.random.randint(N // 4) if i > 0 else 0\n",
    "        signals.append([sig[(starts[i] + a * (2**i)) % N] for a in range(m)])\n",
    "        if (freq * (2**i)) % (N) < N / 2:\n",
    "            vals.append([0])\n",
    "        else:\n",
    "            vals.append([1])\n",
    "    return signals, vals\n",
    "        \n",
    "\n",
    "def make_batch_singleton(batch_size, SNRdb, N, m, default=-1): # 0 = zero, 1 = single, 2 = multi\n",
    "    signals, freqs = [], []\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdB)\n",
    "    for i in range(batch_size):\n",
    "        val = np.random.poisson(0.79)\n",
    "        if default >= 0:\n",
    "            val = default\n",
    "        if val == 0:\n",
    "            signals.append(make_noise(0, m))\n",
    "            freqs.append([1, 0, 0])\n",
    "        if val == 1:\n",
    "            signals.append(make_noisy_signal(2 * np.pi * np.random.randint(0, N) / N, 0, SNRdB, m))\n",
    "            freqs.append([0, 1, 0])\n",
    "        if val >= 2:\n",
    "            signal = make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            for i in range(val - 1):\n",
    "                signal += make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            signals.append(signal + make_noise(sigma2, m))\n",
    "            freqs.append([0, 0, 1])\n",
    "    return signals, freqs\n",
    "\n",
    "def get_sigma2_from_snrdb(SNR_db):\n",
    "    return 10**(-SNR_db/10)\n",
    "\n",
    "def kay_weights(N):\n",
    "    scaling = (3.0/2)*N/(N**2 - 1)\n",
    "    \n",
    "    w = [1 - ((i - (N/2 - 1))/(N/2))**2 for i in range(N-1)]\n",
    "    \n",
    "    return scaling*np.array(w)\n",
    "\n",
    "def kays_method(my_signal):\n",
    "    N = len(my_signal)\n",
    "    w = kay_weights(N)\n",
    "    \n",
    "    angle_diff = np.angle(np.conj(my_signal[0:-1])*my_signal[1:])\n",
    "    need_to_shift = np.any(angle_diff < -np.pi/2)\n",
    "    if need_to_shift:    \n",
    "        neg_idx = angle_diff < 0\n",
    "        angle_diff[neg_idx] += np.pi*2\n",
    "    \n",
    "    return w.dot(angle_diff)\n",
    "\n",
    "def kays_singleton_accuracy(test_signals, test_freqs, N):\n",
    "    diffs = [s - make_signal(kays_method(s), 0, N) for s in test_signals]\n",
    "    thresh, single_acc, other_acc, best_thresh = 0.0, 0, 0, 0\n",
    "    best = 0\n",
    "    for i in range(150):\n",
    "        vals = [(sum(np.absolute(s)) / N) < thresh for s in diffs]\n",
    "        corr = [1 for i in range(len(test_freqs)) if (test_freqs[i] == [0, 1, 0] and vals[i] == 1) or ((test_freqs[i] != [0, 1, 0] and vals[i] == 0))]\n",
    "        corr = sum(corr)\n",
    "        #single = sum([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]])\n",
    "        #other = sum([not vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]])        \n",
    "        #if single*2 + other > single_acc*2 + other_acc and single > 0.2 and other > 0.2:\n",
    "        #    single_acc = single\n",
    "        #    other_acc = other\n",
    "        #    best_thresh = thresh\n",
    "        if corr > best:\n",
    "            best = corr\n",
    "            best_thresh = thresh\n",
    "        thresh += 0.05\n",
    "    print('thresh: ', best_thresh)\n",
    "    return best / len(test_signals)\n",
    "\n",
    "def test_kays(signals, freqs, N):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        res = kays_method(sig)\n",
    "        res = round(res * N / (2 * np.pi))\n",
    "        if np.argmax(freq) == res:\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "\n",
    "def test_mle(signals, freqs, N, m):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        cleans = [make_signal(np.pi * 2 * w / N, 0, m) for w in range(N)]\n",
    "        dots = [np.absolute(np.vdot(sig, clean)) for clean in cleans]\n",
    "        if np.argmax(dots) == np.argmax(freq):\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "    \n",
    "def make_binary(freqs, N):\n",
    "    w = math.ceil(np.log2(N))\n",
    "    return [[int(a) for a in list(np.binary_repr(f, width=w))] for f in freqs] \n",
    "\n",
    "def binary_to_int(binary_string):\n",
    "    return tf.reduce_sum(\n",
    "    tf.cast(tf.reverse(tensor=binary_string, axis=[0]), dtype=tf.int64)\n",
    "    * 2 ** tf.range(tf.cast(tf.size(binary_string), dtype=tf.int64)))\n",
    "    '''y = 0\n",
    "    for i,j in enumerate(x):\n",
    "        y += j<<i\n",
    "    return y'''\n",
    "\n",
    "def hamming(pred, act):\n",
    "    return np.count_nonzero(pred != act)\n",
    "\n",
    "def one_hot(N, batch_size, freqs):\n",
    "    freqs_one_hot = np.zeros((batch_size, N))\n",
    "    freqs_one_hot[np.arange(batch_size), freqs] = 1\n",
    "    return freqs_one_hot\n",
    "\n",
    "def test_noisy_mle(N, m, signals, freqs):\n",
    "    count = 0  \n",
    "    '''imag_signals = []\n",
    "    for index in range(len(signals)):\n",
    "        sig = signals[index]\n",
    "        imag_sig = [(sig[i] + 1j*sig[i+1]) for i in np.arange(len(sig), step=2)]\n",
    "        imag_signals.append(imag_sig)'''\n",
    "    cleans = [make_signal(2*np.pi*i/N, 0, m) for i in range(N)]\n",
    "                     \n",
    "    for index in range(len(signals)):\n",
    "        dots = [np.absolute(np.vdot(signals[index], cleans[i])) for i in range(N)]\n",
    "        if np.argmax(freqs[index]) == np.argmax(dots):\n",
    "            #print(np.argmax(dots))\n",
    "            count += 1\n",
    "    return count / len(freqs)\n",
    "\n",
    "def bit_to_freq(bits, N):\n",
    "    possible = [i for i in range(N)]\n",
    "    for b in bits:\n",
    "        if not b[0]:\n",
    "            possible = possible[:len(possible)//2]\n",
    "        else:\n",
    "            possible = possible[len(possible)//2:]\n",
    "    return possible[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.3787, Training Accuracy= 0.832\n",
      "Iter 1000, Minibatch Loss= 0.1190, Training Accuracy= 0.968\n",
      "Iter 1500, Minibatch Loss= 0.0443, Training Accuracy= 0.986\n",
      "Iter 2000, Minibatch Loss= 0.1002, Training Accuracy= 0.975\n",
      "Iter 2500, Minibatch Loss= 0.1106, Training Accuracy= 0.975\n",
      "Iter 3000, Minibatch Loss= 0.0542, Training Accuracy= 0.979\n",
      "Iter 3500, Minibatch Loss= 0.0195, Training Accuracy= 0.996\n",
      "Iter 4000, Minibatch Loss= 0.0868, Training Accuracy= 0.971\n",
      "Iter 4500, Minibatch Loss= 0.1051, Training Accuracy= 0.979\n",
      "Iter 5000, Minibatch Loss= 0.0431, Training Accuracy= 0.982\n",
      "Iter 5500, Minibatch Loss= 0.0237, Training Accuracy= 0.993\n",
      "Iter 6000, Minibatch Loss= 0.0894, Training Accuracy= 0.975\n",
      "Iter 6500, Minibatch Loss= 0.1057, Training Accuracy= 0.975\n",
      "Iter 7000, Minibatch Loss= 0.0365, Training Accuracy= 0.986\n",
      "Iter 7500, Minibatch Loss= 0.0228, Training Accuracy= 0.993\n",
      "Iter 8000, Minibatch Loss= 0.0830, Training Accuracy= 0.979\n",
      "Iter 8500, Minibatch Loss= 0.1016, Training Accuracy= 0.979\n",
      "Iter 9000, Minibatch Loss= 0.0401, Training Accuracy= 0.986\n",
      "Iter 9500, Minibatch Loss= 0.0252, Training Accuracy= 0.989\n",
      "Iter 10000, Minibatch Loss= 0.0785, Training Accuracy= 0.982\n",
      "Training Finished\n",
      "[0.9928571, 0.9857143, 0.9785714, 0.9892857, 0.9892857, 0.97499996, 0.99642855, 0.9928571, 1.0, 0.9928571]\n",
      "0.88\n",
      "1\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.2370, Training Accuracy= 0.918\n",
      "Iter 1000, Minibatch Loss= 0.1020, Training Accuracy= 0.975\n",
      "Iter 1500, Minibatch Loss= 0.1086, Training Accuracy= 0.968\n",
      "Iter 2000, Minibatch Loss= 0.1089, Training Accuracy= 0.971\n",
      "Iter 2500, Minibatch Loss= 0.0678, Training Accuracy= 0.971\n",
      "Iter 3000, Minibatch Loss= 0.0584, Training Accuracy= 0.986\n",
      "Iter 3500, Minibatch Loss= 0.0970, Training Accuracy= 0.968\n",
      "Iter 4000, Minibatch Loss= 0.0985, Training Accuracy= 0.971\n",
      "Iter 4500, Minibatch Loss= 0.0658, Training Accuracy= 0.971\n",
      "Iter 5000, Minibatch Loss= 0.0557, Training Accuracy= 0.986\n",
      "Iter 5500, Minibatch Loss= 0.0939, Training Accuracy= 0.971\n",
      "Iter 6000, Minibatch Loss= 0.0849, Training Accuracy= 0.968\n",
      "Iter 6500, Minibatch Loss= 0.0605, Training Accuracy= 0.971\n",
      "Iter 7000, Minibatch Loss= 0.0546, Training Accuracy= 0.986\n",
      "Iter 7500, Minibatch Loss= 0.0911, Training Accuracy= 0.975\n",
      "Iter 8000, Minibatch Loss= 0.0722, Training Accuracy= 0.975\n",
      "Iter 8500, Minibatch Loss= 0.0562, Training Accuracy= 0.971\n",
      "Iter 9000, Minibatch Loss= 0.0422, Training Accuracy= 0.982\n",
      "Iter 9500, Minibatch Loss= 0.0767, Training Accuracy= 0.975\n",
      "Iter 10000, Minibatch Loss= 0.0600, Training Accuracy= 0.982\n",
      "Training Finished\n",
      "[0.99285716, 0.98928565, 0.98214287, 0.9892857, 0.9857143, 0.97857153, 0.96071434, 0.99642855, 0.9678572, 0.99285716]\n",
      "0.82\n",
      "2\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.3325, Training Accuracy= 0.875\n",
      "Iter 1000, Minibatch Loss= 0.2712, Training Accuracy= 0.907\n",
      "Iter 1500, Minibatch Loss= 0.1393, Training Accuracy= 0.968\n",
      "Iter 2000, Minibatch Loss= 0.1133, Training Accuracy= 0.961\n",
      "Iter 2500, Minibatch Loss= 0.0816, Training Accuracy= 0.982\n",
      "Iter 3000, Minibatch Loss= 0.0580, Training Accuracy= 0.982\n",
      "Iter 3500, Minibatch Loss= 0.0391, Training Accuracy= 0.986\n",
      "Iter 4000, Minibatch Loss= 0.0423, Training Accuracy= 0.982\n",
      "Iter 4500, Minibatch Loss= 0.0601, Training Accuracy= 0.986\n",
      "Iter 5000, Minibatch Loss= 0.0438, Training Accuracy= 0.982\n",
      "Iter 5500, Minibatch Loss= 0.0344, Training Accuracy= 0.989\n",
      "Iter 6000, Minibatch Loss= 0.0287, Training Accuracy= 0.993\n",
      "Iter 6500, Minibatch Loss= 0.0459, Training Accuracy= 0.986\n",
      "Iter 7000, Minibatch Loss= 0.0388, Training Accuracy= 0.982\n",
      "Iter 7500, Minibatch Loss= 0.0385, Training Accuracy= 0.993\n",
      "Iter 8000, Minibatch Loss= 0.0251, Training Accuracy= 0.993\n",
      "Iter 8500, Minibatch Loss= 0.0416, Training Accuracy= 0.986\n",
      "Iter 9000, Minibatch Loss= 0.0389, Training Accuracy= 0.982\n",
      "Iter 9500, Minibatch Loss= 0.0407, Training Accuracy= 0.993\n",
      "Iter 10000, Minibatch Loss= 0.0234, Training Accuracy= 0.993\n",
      "Training Finished\n",
      "[0.9892857, 0.9928571, 0.99285716, 0.9857142, 1.0, 0.9892857, 0.99285716, 0.98571426, 0.9928571, 0.99285716]\n",
      "0.885\n",
      "3\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6894, Training Accuracy= 0.607\n",
      "Iter 1000, Minibatch Loss= 0.4051, Training Accuracy= 0.796\n",
      "Iter 1500, Minibatch Loss= 0.0955, Training Accuracy= 0.971\n",
      "Iter 2000, Minibatch Loss= 0.0879, Training Accuracy= 0.964\n",
      "Iter 2500, Minibatch Loss= 0.0634, Training Accuracy= 0.979\n",
      "Iter 3000, Minibatch Loss= 0.0217, Training Accuracy= 0.989\n",
      "Iter 3500, Minibatch Loss= 0.0268, Training Accuracy= 0.993\n",
      "Iter 4000, Minibatch Loss= 0.0338, Training Accuracy= 0.979\n",
      "Iter 4500, Minibatch Loss= 0.0401, Training Accuracy= 0.986\n",
      "Iter 5000, Minibatch Loss= 0.0196, Training Accuracy= 0.993\n",
      "Iter 5500, Minibatch Loss= 0.0210, Training Accuracy= 0.989\n",
      "Iter 6000, Minibatch Loss= 0.0304, Training Accuracy= 0.993\n",
      "Iter 6500, Minibatch Loss= 0.0459, Training Accuracy= 0.982\n",
      "Iter 7000, Minibatch Loss= 0.0232, Training Accuracy= 0.986\n",
      "Iter 7500, Minibatch Loss= 0.0213, Training Accuracy= 0.993\n",
      "Iter 8000, Minibatch Loss= 0.0296, Training Accuracy= 0.989\n",
      "Iter 8500, Minibatch Loss= 0.0436, Training Accuracy= 0.986\n",
      "Iter 9000, Minibatch Loss= 0.0256, Training Accuracy= 0.986\n",
      "Iter 9500, Minibatch Loss= 0.0206, Training Accuracy= 0.993\n",
      "Iter 10000, Minibatch Loss= 0.0304, Training Accuracy= 0.989\n",
      "Training Finished\n",
      "[0.9928571, 0.9928571, 0.98928577, 0.9892857, 0.9714286, 0.9892857, 0.9785714, 0.98928565, 0.98928565, 0.9892857]\n",
      "0.875\n",
      "0.9928571\n",
      "0.885\n",
      "2\n",
      "0\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.3977, Training Accuracy= 0.836\n",
      "Iter 1000, Minibatch Loss= 0.1874, Training Accuracy= 0.943\n",
      "Iter 1500, Minibatch Loss= 0.1393, Training Accuracy= 0.961\n",
      "Iter 2000, Minibatch Loss= 0.1192, Training Accuracy= 0.961\n",
      "Iter 2500, Minibatch Loss= 0.2622, Training Accuracy= 0.918\n",
      "Iter 3000, Minibatch Loss= 0.1362, Training Accuracy= 0.954\n",
      "Iter 3500, Minibatch Loss= 0.1048, Training Accuracy= 0.961\n",
      "Iter 4000, Minibatch Loss= 0.1123, Training Accuracy= 0.968\n",
      "Iter 4500, Minibatch Loss= 0.2067, Training Accuracy= 0.939\n",
      "Iter 5000, Minibatch Loss= 0.1428, Training Accuracy= 0.961\n",
      "Iter 5500, Minibatch Loss= 0.0821, Training Accuracy= 0.968\n",
      "Iter 6000, Minibatch Loss= 0.1180, Training Accuracy= 0.968\n",
      "Iter 6500, Minibatch Loss= 0.1764, Training Accuracy= 0.939\n",
      "Iter 7000, Minibatch Loss= 0.1494, Training Accuracy= 0.968\n",
      "Iter 7500, Minibatch Loss= 0.0741, Training Accuracy= 0.968\n",
      "Iter 8000, Minibatch Loss= 0.1269, Training Accuracy= 0.964\n",
      "Iter 8500, Minibatch Loss= 0.1700, Training Accuracy= 0.943\n",
      "Iter 9000, Minibatch Loss= 0.1346, Training Accuracy= 0.964\n",
      "Iter 9500, Minibatch Loss= 0.0615, Training Accuracy= 0.979\n",
      "Iter 10000, Minibatch Loss= 0.1128, Training Accuracy= 0.968\n",
      "Training Finished\n",
      "[0.97499996, 0.9642857, 0.9785714, 0.975, 0.9821428, 0.9714286, 0.98214287, 0.98928577, 0.9857143, 0.95714283]\n",
      "0.75\n",
      "1\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6329, Training Accuracy= 0.686\n",
      "Iter 1000, Minibatch Loss= 0.5648, Training Accuracy= 0.725\n",
      "Iter 1500, Minibatch Loss= 0.5673, Training Accuracy= 0.714\n",
      "Iter 2000, Minibatch Loss= 0.3511, Training Accuracy= 0.843\n",
      "Iter 2500, Minibatch Loss= 0.2661, Training Accuracy= 0.893\n",
      "Iter 3000, Minibatch Loss= 0.1175, Training Accuracy= 0.968\n",
      "Iter 3500, Minibatch Loss= 0.0773, Training Accuracy= 0.979\n",
      "Iter 4000, Minibatch Loss= 0.0800, Training Accuracy= 0.979\n",
      "Iter 4500, Minibatch Loss= 0.1682, Training Accuracy= 0.929\n",
      "Iter 5000, Minibatch Loss= 0.0961, Training Accuracy= 0.968\n",
      "Iter 5500, Minibatch Loss= 0.0521, Training Accuracy= 0.989\n",
      "Iter 6000, Minibatch Loss= 0.0709, Training Accuracy= 0.982\n",
      "Iter 6500, Minibatch Loss= 0.1334, Training Accuracy= 0.943\n",
      "Iter 7000, Minibatch Loss= 0.1005, Training Accuracy= 0.968\n",
      "Iter 7500, Minibatch Loss= 0.0530, Training Accuracy= 0.982\n",
      "Iter 8000, Minibatch Loss= 0.0602, Training Accuracy= 0.979\n",
      "Iter 8500, Minibatch Loss= 0.1371, Training Accuracy= 0.950\n",
      "Iter 9000, Minibatch Loss= 0.1052, Training Accuracy= 0.961\n",
      "Iter 9500, Minibatch Loss= 0.0552, Training Accuracy= 0.986\n",
      "Iter 10000, Minibatch Loss= 0.0616, Training Accuracy= 0.982\n",
      "Training Finished\n",
      "[0.9857142, 0.9857143, 0.9785715, 0.95000005, 0.9678572, 0.9892857, 0.9785714, 0.9892857, 0.96785724, 0.9571429]\n",
      "0.76\n",
      "2\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6917, Training Accuracy= 0.486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Minibatch Loss= 0.6627, Training Accuracy= 0.575\n",
      "Iter 1500, Minibatch Loss= 0.5468, Training Accuracy= 0.732\n",
      "Iter 2000, Minibatch Loss= 0.2772, Training Accuracy= 0.889\n",
      "Iter 2500, Minibatch Loss= 0.4141, Training Accuracy= 0.846\n",
      "Iter 3000, Minibatch Loss= 0.2463, Training Accuracy= 0.921\n",
      "Iter 3500, Minibatch Loss= 0.1529, Training Accuracy= 0.954\n",
      "Iter 4000, Minibatch Loss= 0.0789, Training Accuracy= 0.979\n",
      "Iter 4500, Minibatch Loss= 0.2027, Training Accuracy= 0.932\n",
      "Iter 5000, Minibatch Loss= 0.0934, Training Accuracy= 0.968\n",
      "Iter 5500, Minibatch Loss= 0.0805, Training Accuracy= 0.968\n",
      "Iter 6000, Minibatch Loss= 0.0594, Training Accuracy= 0.982\n",
      "Iter 6500, Minibatch Loss= 0.1532, Training Accuracy= 0.943\n",
      "Iter 7000, Minibatch Loss= 0.0841, Training Accuracy= 0.971\n",
      "Iter 7500, Minibatch Loss= 0.0682, Training Accuracy= 0.971\n",
      "Iter 8000, Minibatch Loss= 0.0739, Training Accuracy= 0.971\n",
      "Iter 8500, Minibatch Loss= 0.1564, Training Accuracy= 0.943\n",
      "Iter 9000, Minibatch Loss= 0.0769, Training Accuracy= 0.971\n",
      "Iter 9500, Minibatch Loss= 0.0671, Training Accuracy= 0.975\n",
      "Iter 10000, Minibatch Loss= 0.0747, Training Accuracy= 0.968\n",
      "Training Finished\n",
      "[0.9857142, 0.96428573, 0.98571426, 0.9714286, 0.9892857, 0.98571426, 0.9821428, 0.96071434, 0.98928565, 0.98571426]\n",
      "0.81\n",
      "3\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.5290, Training Accuracy= 0.746\n",
      "Iter 1000, Minibatch Loss= 0.3266, Training Accuracy= 0.871\n",
      "Iter 1500, Minibatch Loss= 0.2499, Training Accuracy= 0.893\n",
      "Iter 2000, Minibatch Loss= 0.1127, Training Accuracy= 0.964\n",
      "Iter 2500, Minibatch Loss= 0.2148, Training Accuracy= 0.918\n",
      "Iter 3000, Minibatch Loss= 0.1127, Training Accuracy= 0.961\n",
      "Iter 3500, Minibatch Loss= 0.1246, Training Accuracy= 0.950\n",
      "Iter 4000, Minibatch Loss= 0.0628, Training Accuracy= 0.986\n",
      "Iter 4500, Minibatch Loss= 0.1318, Training Accuracy= 0.950\n",
      "Iter 5000, Minibatch Loss= 0.0811, Training Accuracy= 0.971\n",
      "Iter 5500, Minibatch Loss= 0.0696, Training Accuracy= 0.968\n",
      "Iter 6000, Minibatch Loss= 0.0511, Training Accuracy= 0.986\n",
      "Iter 6500, Minibatch Loss= 0.1091, Training Accuracy= 0.957\n",
      "Iter 7000, Minibatch Loss= 0.0720, Training Accuracy= 0.971\n",
      "Iter 7500, Minibatch Loss= 0.0675, Training Accuracy= 0.979\n",
      "Iter 8000, Minibatch Loss= 0.0584, Training Accuracy= 0.971\n",
      "Iter 8500, Minibatch Loss= 0.1121, Training Accuracy= 0.957\n",
      "Iter 9000, Minibatch Loss= 0.0594, Training Accuracy= 0.982\n",
      "Iter 9500, Minibatch Loss= 0.0646, Training Accuracy= 0.979\n",
      "Iter 10000, Minibatch Loss= 0.0632, Training Accuracy= 0.975\n",
      "Training Finished\n",
      "[0.9928571, 0.98214287, 0.9571429, 0.98214287, 0.9714285, 0.98928565, 0.9678571, 0.97142863, 0.9714286, 0.9678571]\n",
      "0.765\n",
      "0.9857142\n",
      "0.81\n",
      "0\n",
      "0\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6931, Training Accuracy= 0.493\n",
      "Iter 1000, Minibatch Loss= 0.6769, Training Accuracy= 0.561\n",
      "Iter 1500, Minibatch Loss= 0.6252, Training Accuracy= 0.689\n",
      "Iter 2000, Minibatch Loss= 0.6440, Training Accuracy= 0.614\n",
      "Iter 2500, Minibatch Loss= 0.5205, Training Accuracy= 0.736\n",
      "Iter 3000, Minibatch Loss= 0.4450, Training Accuracy= 0.804\n",
      "Iter 3500, Minibatch Loss= 0.3296, Training Accuracy= 0.879\n",
      "Iter 4000, Minibatch Loss= 0.3141, Training Accuracy= 0.879\n",
      "Iter 4500, Minibatch Loss= 0.2771, Training Accuracy= 0.879\n",
      "Iter 5000, Minibatch Loss= 0.2930, Training Accuracy= 0.868\n",
      "Iter 5500, Minibatch Loss= 0.2643, Training Accuracy= 0.907\n",
      "Iter 6000, Minibatch Loss= 0.2961, Training Accuracy= 0.896\n",
      "Iter 6500, Minibatch Loss= 0.2614, Training Accuracy= 0.889\n",
      "Iter 7000, Minibatch Loss= 0.2907, Training Accuracy= 0.864\n",
      "Iter 7500, Minibatch Loss= 0.2660, Training Accuracy= 0.893\n",
      "Iter 8000, Minibatch Loss= 0.2939, Training Accuracy= 0.886\n",
      "Iter 8500, Minibatch Loss= 0.2621, Training Accuracy= 0.879\n",
      "Iter 9000, Minibatch Loss= 0.2920, Training Accuracy= 0.864\n",
      "Iter 9500, Minibatch Loss= 0.2641, Training Accuracy= 0.893\n",
      "Iter 10000, Minibatch Loss= 0.2925, Training Accuracy= 0.886\n",
      "Training Finished\n",
      "[0.8535714, 0.8607143, 0.83928573, 0.9107143, 0.86785716, 0.88928574, 0.8642857, 0.825, 0.8357143, 0.82142854]\n",
      "0.165\n",
      "1\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6938, Training Accuracy= 0.564\n",
      "Iter 1000, Minibatch Loss= 0.6941, Training Accuracy= 0.468\n",
      "Iter 1500, Minibatch Loss= 0.6110, Training Accuracy= 0.636\n",
      "Iter 2000, Minibatch Loss= 0.3529, Training Accuracy= 0.861\n",
      "Iter 2500, Minibatch Loss= 0.3681, Training Accuracy= 0.854\n",
      "Iter 3000, Minibatch Loss= 0.1417, Training Accuracy= 0.957\n",
      "Iter 3500, Minibatch Loss= 0.1451, Training Accuracy= 0.943\n",
      "Iter 4000, Minibatch Loss= 0.1389, Training Accuracy= 0.961\n",
      "Iter 4500, Minibatch Loss= 0.1381, Training Accuracy= 0.961\n",
      "Iter 5000, Minibatch Loss= 0.0898, Training Accuracy= 0.975\n",
      "Iter 5500, Minibatch Loss= 0.1203, Training Accuracy= 0.954\n",
      "Iter 6000, Minibatch Loss= 0.1250, Training Accuracy= 0.957\n",
      "Iter 6500, Minibatch Loss= 0.1496, Training Accuracy= 0.950\n",
      "Iter 7000, Minibatch Loss= 0.0822, Training Accuracy= 0.979\n",
      "Iter 7500, Minibatch Loss= 0.1143, Training Accuracy= 0.957\n",
      "Iter 8000, Minibatch Loss= 0.1157, Training Accuracy= 0.961\n",
      "Iter 8500, Minibatch Loss= 0.1398, Training Accuracy= 0.957\n",
      "Iter 9000, Minibatch Loss= 0.0807, Training Accuracy= 0.979\n",
      "Iter 9500, Minibatch Loss= 0.1139, Training Accuracy= 0.950\n",
      "Iter 10000, Minibatch Loss= 0.1082, Training Accuracy= 0.968\n",
      "Training Finished\n",
      "[0.9678571, 0.96428573, 0.95357144, 0.96071434, 0.9571429, 0.9607143, 0.96428573, 0.9714285, 0.9678571, 0.93928576]\n",
      "0.575\n",
      "2\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6999, Training Accuracy= 0.496\n",
      "Iter 1000, Minibatch Loss= 0.6672, Training Accuracy= 0.518\n",
      "Iter 1500, Minibatch Loss= 0.5906, Training Accuracy= 0.714\n",
      "Iter 2000, Minibatch Loss= 0.3519, Training Accuracy= 0.868\n",
      "Iter 2500, Minibatch Loss= 0.2667, Training Accuracy= 0.893\n",
      "Iter 3000, Minibatch Loss= 0.1194, Training Accuracy= 0.968\n",
      "Iter 3500, Minibatch Loss= 0.0869, Training Accuracy= 0.968\n",
      "Iter 4000, Minibatch Loss= 0.1127, Training Accuracy= 0.957\n",
      "Iter 4500, Minibatch Loss= 0.0796, Training Accuracy= 0.971\n",
      "Iter 5000, Minibatch Loss= 0.0838, Training Accuracy= 0.975\n",
      "Iter 5500, Minibatch Loss= 0.0832, Training Accuracy= 0.975\n",
      "Iter 6000, Minibatch Loss= 0.1106, Training Accuracy= 0.954\n",
      "Iter 6500, Minibatch Loss= 0.0854, Training Accuracy= 0.957\n",
      "Iter 7000, Minibatch Loss= 0.0809, Training Accuracy= 0.979\n",
      "Iter 7500, Minibatch Loss= 0.0866, Training Accuracy= 0.968\n",
      "Iter 8000, Minibatch Loss= 0.1128, Training Accuracy= 0.954\n",
      "Iter 8500, Minibatch Loss= 0.0871, Training Accuracy= 0.961\n",
      "Iter 9000, Minibatch Loss= 0.0806, Training Accuracy= 0.979\n",
      "Iter 9500, Minibatch Loss= 0.0872, Training Accuracy= 0.968\n",
      "Iter 10000, Minibatch Loss= 0.1141, Training Accuracy= 0.954\n",
      "Training Finished\n",
      "[0.95357144, 0.9571429, 0.96428573, 0.9678571, 0.95714283, 0.9714286, 0.9678571, 0.9321428, 0.9678571, 0.95357144]\n",
      "0.64\n",
      "3\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6537, Training Accuracy= 0.618\n",
      "Iter 1000, Minibatch Loss= 0.3419, Training Accuracy= 0.886\n",
      "Iter 1500, Minibatch Loss= 0.3014, Training Accuracy= 0.886\n",
      "Iter 2000, Minibatch Loss= 0.2555, Training Accuracy= 0.911\n",
      "Iter 2500, Minibatch Loss= 0.2358, Training Accuracy= 0.921\n",
      "Iter 3000, Minibatch Loss= 0.2119, Training Accuracy= 0.939\n",
      "Iter 3500, Minibatch Loss= 0.2460, Training Accuracy= 0.918\n",
      "Iter 4000, Minibatch Loss= 0.2057, Training Accuracy= 0.932\n",
      "Iter 4500, Minibatch Loss= 0.2127, Training Accuracy= 0.918\n",
      "Iter 5000, Minibatch Loss= 0.1950, Training Accuracy= 0.957\n",
      "Iter 5500, Minibatch Loss= 0.2293, Training Accuracy= 0.921\n",
      "Iter 6000, Minibatch Loss= 0.1970, Training Accuracy= 0.939\n",
      "Iter 6500, Minibatch Loss= 0.2073, Training Accuracy= 0.918\n",
      "Iter 7000, Minibatch Loss= 0.1956, Training Accuracy= 0.957\n",
      "Iter 7500, Minibatch Loss= 0.2070, Training Accuracy= 0.936\n",
      "Iter 8000, Minibatch Loss= 0.1918, Training Accuracy= 0.943\n",
      "Iter 8500, Minibatch Loss= 0.2024, Training Accuracy= 0.914\n",
      "Iter 9000, Minibatch Loss= 0.2100, Training Accuracy= 0.946\n",
      "Iter 9500, Minibatch Loss= 0.2032, Training Accuracy= 0.939\n",
      "Iter 10000, Minibatch Loss= 0.1910, Training Accuracy= 0.943\n",
      "Training Finished\n",
      "[0.92857146, 0.95714283, 0.92142856, 0.95000005, 0.925, 0.92857146, 0.9142857, 0.93571424, 0.93571424, 0.9000001]\n",
      "0.39\n",
      "0.96250004\n",
      "0.64\n",
      "-1\n",
      "0\n",
      "0\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.5997, Training Accuracy= 0.675\n",
      "Iter 1000, Minibatch Loss= 0.4871, Training Accuracy= 0.786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1500, Minibatch Loss= 0.4659, Training Accuracy= 0.793\n",
      "Iter 2000, Minibatch Loss= 0.3798, Training Accuracy= 0.843\n",
      "Iter 2500, Minibatch Loss= 0.2920, Training Accuracy= 0.875\n",
      "Iter 3000, Minibatch Loss= 0.4205, Training Accuracy= 0.818\n",
      "Iter 3500, Minibatch Loss= 0.4823, Training Accuracy= 0.800\n",
      "Iter 4000, Minibatch Loss= 0.3718, Training Accuracy= 0.825\n",
      "Iter 4500, Minibatch Loss= 0.2812, Training Accuracy= 0.879\n",
      "Iter 5000, Minibatch Loss= 0.4147, Training Accuracy= 0.811\n",
      "Iter 5500, Minibatch Loss= 0.4664, Training Accuracy= 0.789\n",
      "Iter 6000, Minibatch Loss= 0.3265, Training Accuracy= 0.882\n",
      "Iter 6500, Minibatch Loss= 0.2510, Training Accuracy= 0.914\n",
      "Iter 7000, Minibatch Loss= 0.3378, Training Accuracy= 0.868\n",
      "Iter 7500, Minibatch Loss= 0.4091, Training Accuracy= 0.839\n",
      "Iter 8000, Minibatch Loss= 0.3738, Training Accuracy= 0.861\n",
      "Iter 8500, Minibatch Loss= 0.2497, Training Accuracy= 0.907\n",
      "Iter 9000, Minibatch Loss= 0.3913, Training Accuracy= 0.854\n",
      "Iter 9500, Minibatch Loss= 0.3244, Training Accuracy= 0.886\n",
      "Iter 10000, Minibatch Loss= 0.2531, Training Accuracy= 0.900\n",
      "Training Finished\n",
      "[0.9285714, 0.8964286, 0.9464286, 0.9071429, 0.9285715, 0.89285713, 0.9035715, 0.90000004, 0.8964286, 0.9428572]\n",
      "0.33\n",
      "1\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6791, Training Accuracy= 0.532\n",
      "Iter 1000, Minibatch Loss= 0.6934, Training Accuracy= 0.489\n",
      "Iter 1500, Minibatch Loss= 0.6938, Training Accuracy= 0.489\n",
      "Iter 2000, Minibatch Loss= 0.6935, Training Accuracy= 0.482\n",
      "Iter 2500, Minibatch Loss= 0.6880, Training Accuracy= 0.504\n",
      "Iter 3000, Minibatch Loss= 0.6933, Training Accuracy= 0.532\n",
      "Iter 3500, Minibatch Loss= 0.4703, Training Accuracy= 0.800\n",
      "Iter 4000, Minibatch Loss= 0.3061, Training Accuracy= 0.889\n",
      "Iter 4500, Minibatch Loss= 0.2716, Training Accuracy= 0.914\n",
      "Iter 5000, Minibatch Loss= 0.3242, Training Accuracy= 0.882\n",
      "Iter 5500, Minibatch Loss= 0.3730, Training Accuracy= 0.889\n",
      "Iter 6000, Minibatch Loss= 0.2672, Training Accuracy= 0.914\n",
      "Iter 6500, Minibatch Loss= 0.2251, Training Accuracy= 0.929\n",
      "Iter 7000, Minibatch Loss= 0.3060, Training Accuracy= 0.893\n",
      "Iter 7500, Minibatch Loss= 0.3453, Training Accuracy= 0.882\n",
      "Iter 8000, Minibatch Loss= 0.2732, Training Accuracy= 0.918\n",
      "Iter 8500, Minibatch Loss= 0.2015, Training Accuracy= 0.936\n",
      "Iter 9000, Minibatch Loss= 0.2928, Training Accuracy= 0.893\n",
      "Iter 9500, Minibatch Loss= 0.3339, Training Accuracy= 0.893\n",
      "Iter 10000, Minibatch Loss= 0.2751, Training Accuracy= 0.914\n",
      "Training Finished\n",
      "[0.9214287, 0.9250001, 0.9357143, 0.8714286, 0.92857146, 0.93214285, 0.9107143, 0.9035714, 0.93928564, 0.9071429]\n",
      "0.325\n",
      "2\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.7136, Training Accuracy= 0.507\n",
      "Iter 1000, Minibatch Loss= 0.6929, Training Accuracy= 0.525\n",
      "Iter 1500, Minibatch Loss= 0.7063, Training Accuracy= 0.539\n",
      "Iter 2000, Minibatch Loss= 0.6960, Training Accuracy= 0.504\n",
      "Iter 2500, Minibatch Loss= 0.6865, Training Accuracy= 0.489\n",
      "Iter 3000, Minibatch Loss= 0.6912, Training Accuracy= 0.521\n",
      "Iter 3500, Minibatch Loss= 0.7113, Training Accuracy= 0.521\n",
      "Iter 4000, Minibatch Loss= 0.7104, Training Accuracy= 0.557\n",
      "Iter 4500, Minibatch Loss= 0.5563, Training Accuracy= 0.686\n",
      "Iter 5000, Minibatch Loss= 0.4653, Training Accuracy= 0.782\n",
      "Iter 5500, Minibatch Loss= 0.4407, Training Accuracy= 0.789\n",
      "Iter 6000, Minibatch Loss= 0.3818, Training Accuracy= 0.818\n",
      "Iter 6500, Minibatch Loss= 0.3436, Training Accuracy= 0.868\n",
      "Iter 7000, Minibatch Loss= 0.3225, Training Accuracy= 0.893\n",
      "Iter 7500, Minibatch Loss= 0.3801, Training Accuracy= 0.868\n",
      "Iter 8000, Minibatch Loss= 0.2873, Training Accuracy= 0.893\n",
      "Iter 8500, Minibatch Loss= 0.2620, Training Accuracy= 0.929\n",
      "Iter 9000, Minibatch Loss= 0.3319, Training Accuracy= 0.896\n",
      "Iter 9500, Minibatch Loss= 0.3900, Training Accuracy= 0.879\n",
      "Iter 10000, Minibatch Loss= 0.2759, Training Accuracy= 0.886\n",
      "Training Finished\n",
      "[0.86785716, 0.925, 0.9071429, 0.9035714, 0.8892857, 0.92857146, 0.925, 0.9035715, 0.9142858, 0.8964286]\n",
      "0.32\n",
      "3\n",
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6380, Training Accuracy= 0.639\n",
      "Iter 1000, Minibatch Loss= 0.6134, Training Accuracy= 0.689\n",
      "Iter 1500, Minibatch Loss= 0.5397, Training Accuracy= 0.739\n",
      "Iter 2000, Minibatch Loss= 0.4028, Training Accuracy= 0.836\n",
      "Iter 2500, Minibatch Loss= 0.2107, Training Accuracy= 0.925\n",
      "Iter 3000, Minibatch Loss= 0.2857, Training Accuracy= 0.907\n",
      "Iter 3500, Minibatch Loss= 0.2356, Training Accuracy= 0.918\n",
      "Iter 4000, Minibatch Loss= 0.1657, Training Accuracy= 0.950\n",
      "Iter 4500, Minibatch Loss= 0.1152, Training Accuracy= 0.971\n",
      "Iter 5000, Minibatch Loss= 0.2058, Training Accuracy= 0.929\n",
      "Iter 5500, Minibatch Loss= 0.1873, Training Accuracy= 0.936\n",
      "Iter 6000, Minibatch Loss= 0.1384, Training Accuracy= 0.964\n",
      "Iter 6500, Minibatch Loss= 0.1143, Training Accuracy= 0.975\n",
      "Iter 7000, Minibatch Loss= 0.1919, Training Accuracy= 0.929\n",
      "Iter 7500, Minibatch Loss= 0.1776, Training Accuracy= 0.936\n",
      "Iter 8000, Minibatch Loss= 0.1313, Training Accuracy= 0.957\n",
      "Iter 8500, Minibatch Loss= 0.1101, Training Accuracy= 0.971\n",
      "Iter 9000, Minibatch Loss= 0.1823, Training Accuracy= 0.932\n",
      "Iter 9500, Minibatch Loss= 0.1751, Training Accuracy= 0.946\n",
      "Iter 10000, Minibatch Loss= 0.1274, Training Accuracy= 0.957\n",
      "Training Finished\n",
      "[0.96071434, 0.9428571, 0.93571436, 0.93571436, 0.975, 0.96071434, 0.95357144, 0.94285715, 0.94642854, 0.96071434]\n",
      "0.565\n",
      "0.95\n",
      "0.565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"np.save('./data/testing/snrs', snrs)   \\nnp.save('./data/testing/acc_binary', bin_accs)\\nnp.save('./data/testing/acc_frequency', freq_accs)\\nnp.save('./data/testing/preds_frequency', all_preds)\\nnp.save('./data/testing/actuals_frequency', all_actuals)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# div and conquer freq detect\n",
    "\n",
    "N = 16384 \n",
    "layer = 2    #6\n",
    "m = 30\n",
    "\n",
    "log = int(np.log2(N))\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "num_iter = 10000 #80000\n",
    "batch_size = log * 20\n",
    "\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_classes = 1\n",
    "\n",
    "#snrs = [10, 8, 6, 4, 2, 0, -2]\n",
    "snrs = [4, 2, 0, -1]\n",
    "\n",
    "bin_accs = []\n",
    "freq_accs = []\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "\n",
    "#starts = [0]\n",
    "#for i in range(1, log): # filling all the starting points\n",
    "#    starts.append(starts[i-1] + np.random.randint(N // 4))\n",
    "\n",
    "starts = [0] * 15\n",
    "\n",
    "for SNRdB in snrs:\n",
    "    print(SNRdB)\n",
    "\n",
    "    t_bins = []\n",
    "    t_freqs = []\n",
    "    t_preds = []\n",
    "    t_actuals = []\n",
    "    \n",
    "    training_size = 400 #5999\n",
    "    dict = {}\n",
    "    for i in range(training_size):\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "        batch_x, batch_y, batch_freqs = make_batch_noisy_lohi(batch_size // log, SNRdB - (i % 3), N, m, starts)\n",
    "        batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "        batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "        batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "        dict[i] = (batch_x_pair, batch_y)\n",
    "    \n",
    "    for trial in range(4):\n",
    "        print(trial)\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, m, 2])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        # Store layers weight & bias\n",
    "        weights = {i: tf.Variable(tf.random_normal([3, 2, 2])) for i in range(1, layer+1)} # increase out channels, less layers\n",
    "        weights[0] = tf.Variable(tf.random_normal([5, 2, 2]))\n",
    "        weights['out'] = tf.Variable(tf.random_normal([(m-4-(0*layer))*2, num_classes]))\n",
    "        biases = {i: tf.Variable(tf.random_normal([2])) for i in range(layer+1)}\n",
    "        biases['out'] = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights[0], 1, 'VALID'), biases[0])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            for i in range(1, layer+1):\n",
    "                layer_1 = tf.add(tf.nn.conv1d(hidden_1, weights[i], 1, 'SAME'), biases[i]) # no padding\n",
    "                hidden_1 = tf.nn.relu(layer_1) # try: elu, leaky\n",
    "                ### instance normalize\n",
    "            hidden_3 = tf.reshape(hidden_1, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "\n",
    "\n",
    "        test_dict = {}\n",
    "        for i in range(10):\n",
    "            test_signals, test_freqs, freqs = make_batch_noisy_lohi(batch_size // log, SNRdB, N, m, starts)\n",
    "            test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "            test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "            test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "            test_dict[i] = (test_signals_pair, test_freqs, freqs)\n",
    "\n",
    "\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net(X)\n",
    "        prediction = tf.nn.sigmoid(logits)\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))  \n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "        # Evaluate model\n",
    "        pred_class = tf.greater(prediction, 0.5)\n",
    "        correct_pred = tf.equal(pred_class, tf.equal(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "            print(\"Training Started\")\n",
    "\n",
    "            for step in range(1, num_iter + 1):\n",
    "                batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "                if step % 500 == 0:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "\n",
    "                    accuracies.append(acc)\n",
    "                    losses.append(loss)\n",
    "                    print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "            print(\"Training Finished\")\n",
    "            preds = [sess.run(prediction, feed_dict={X: test_dict[i][0], Y: test_dict[i][1]}) for i in range(len(test_dict))]  \n",
    "            nn_acc = [sess.run(accuracy, feed_dict={X: test_dict[i][0], Y: test_dict[i][1]}) for i in range(len(test_dict))]   \n",
    "            print(nn_acc)\n",
    "        t_preds.append(preds)\n",
    "        t_actuals.append([test_dict[i][1] for i in range(len(test_dict))])\n",
    "\n",
    "        t_bins.append(np.median(nn_acc))\n",
    "        preds = np.round(preds)\n",
    "        corr = []\n",
    "        for a in range(len(test_dict)):\n",
    "            fs = []\n",
    "            for k in range(len(preds[a]) // log):\n",
    "                    fs.append(bit_to_freq(preds[a][k * log : (k+1) * log], N))\n",
    "            corr.extend([fs[i] == test_dict[a][2][i] % N for i in range(len(fs))])\n",
    "        t_freqs.append(np.sum(corr) / (len(fs) * len(test_dict)))\n",
    "        print(t_freqs[-1])\n",
    "    bin_accs.append(max(t_bins))\n",
    "    freq_accs.append(max(t_freqs))\n",
    "    all_preds.append(t_preds[np.argmax(t_freqs)])\n",
    "    all_actuals.append(t_actuals[np.argmax(t_freqs)])\n",
    "    print(bin_accs[-1])\n",
    "    print(freq_accs[-1])\n",
    "    \n",
    "'''np.save('./data/testing/snrs', snrs)   \n",
    "np.save('./data/testing/acc_binary', bin_accs)\n",
    "np.save('./data/testing/acc_frequency', freq_accs)\n",
    "np.save('./data/testing/preds_frequency', all_preds)\n",
    "np.save('./data/testing/actuals_frequency', all_actuals)'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34\n"
     ]
    }
   ],
   "source": [
    "preds = np.round(preds)\n",
    "corr = []\n",
    "for a in range(len(test_dict)):\n",
    "    fs = []\n",
    "    for k in range(len(preds[a]) // log):\n",
    "            fs.append(bit_to_freq(preds[a][k * log : (k+1) * log], N))\n",
    "    corr.extend([fs[i] == test_dict[a][2][i] % N for i in range(len(fs))])\n",
    "print(np.sum(corr) / (len(fs) * len(test_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303,)\n",
      "499\n",
      "[1 0 0 0 0 0 1 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.load('./data/testing/acc_binary.npy')\n",
    "freqs = np.load('./data/testing/acc_frequency.npy')\n",
    "all_preds = np.load('./data/testing/preds_frequency.npy')\n",
    "all_actuals = np.load('./data/testing/actuals_frequency.npy')\n",
    "preds = np.reshape(all_preds, -1)\n",
    "actuals = np.reshape(all_actuals, -1)\n",
    "wrongs = []\n",
    "for p, a in zip(preds, actuals):\n",
    "    if int(np.round(p)) != int(a):\n",
    "        wrongs.append(p)\n",
    "print(np.shape(wrongs))\n",
    "print(bit_to_freq(np.expand_dims(actuals[0:10], 1), 1024))\n",
    "print(actuals[0:10])\n",
    "plt.hist(wrongs)\n",
    "plt.show()\n",
    "hammings = []\n",
    "for i in range(len(preds) // 10):\n",
    "    hammings.append(np.sum([int(round(preds[i*10 + j])) != int(actuals[i*10 + j]) for j in range(10)]))\n",
    "plt.hist(hammings)\n",
    "plt.show()\n",
    "# bit to freq is just inverse of binary representation for freq - use to calc hamming and confidence vs pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
