{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signal(w,theta,n):\n",
    "    \"\"\"\n",
    "    Assumes normalized amplitude\n",
    "    \"\"\"\n",
    "    t = np.arange(n)\n",
    "    signal = np.exp(1j*(w*t + theta))\n",
    "    return signal\n",
    "\n",
    "def make_noise(sigma2,n):\n",
    "    noise_scaling = np.sqrt(sigma2/2)\n",
    "    # noise is complex valued\n",
    "    noise  = noise_scaling*np.random.randn(n) + 1j*noise_scaling*np.random.randn(n)\n",
    "    return noise\n",
    "\n",
    "def make_noisy_signal(w,theta,SNRdb,n):\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdb)\n",
    "    signal = make_signal(w,theta,n)\n",
    "    noise  = make_noise(sigma2,n)\n",
    "    return signal + noise\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "def make_batch_noisy(batch_size, SNRdb, N, m, binary=False):\n",
    "    signals, freqs = [], []\n",
    "    for i in range(batch_size):\n",
    "        freq = np.random.randint(0, N)\n",
    "        w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "        sig = make_noisy_signal(w, 0, SNRdb, m)\n",
    "        signals.append(sig)\n",
    "        freqs.append(freq)\n",
    "    if binary:\n",
    "        return signals, make_binary(freqs, N), one_hot(N, batch_size, freqs)\n",
    "    return signals, one_hot(N, batch_size, freqs)\n",
    "\n",
    "# N = divisor of w0\n",
    "# m = num samples\n",
    "def make_batch_noisy_lohi(batch_size, SNRdb, N, m):\n",
    "    freqs = []\n",
    "    freqs.append(np.random.randint(0, N))\n",
    "    test_signals, test_freqs = make_noisy_lohi(SNRdB, N, m, freqs[-1])\n",
    "    for i in range(1, batch_size):\n",
    "        freqs.append(np.random.randint(0, N))\n",
    "        a, b = make_noisy_lohi(SNRdB, N, m, freqs[-1])\n",
    "        test_signals.extend(a)\n",
    "        test_freqs.extend(b)\n",
    "    return test_signals, test_freqs, freqs\n",
    "\n",
    "def make_noisy_lohi(SNRdb, N, m, freq):\n",
    "    signals, vals = [], []\n",
    "    steps = int(np.log2(N))\n",
    "    w = (2 * np.pi * freq / N) % (2 * np.pi)\n",
    "    sig = make_noisy_signal(w, 0, SNRdb, m * (2**steps))\n",
    "    for i in range(int(np.log2(N))):\n",
    "        signals.append([sig[a * (2**i)] for a in range(m)])\n",
    "        if (freq * (2**i)) % (N) < N / 2:\n",
    "            vals.append([1])\n",
    "        else:\n",
    "            vals.append([0])\n",
    "    return signals, vals\n",
    "        \n",
    "\n",
    "def make_batch_singleton(batch_size, SNRdb, N, m, default=-1): # 0 = zero, 1 = single, 2 = multi\n",
    "    signals, freqs = [], []\n",
    "    sigma2 = get_sigma2_from_snrdb(SNRdB)\n",
    "    for i in range(batch_size):\n",
    "        val = np.random.poisson(0.79)\n",
    "        if default >= 0:\n",
    "            val = default\n",
    "        if val == 0:\n",
    "            signals.append(make_noise(0, m))\n",
    "            freqs.append([1, 0, 0])\n",
    "        if val == 1:\n",
    "            signals.append(make_noisy_signal(2 * np.pi * np.random.randint(0, N) / N, 0, SNRdB, m))\n",
    "            freqs.append([0, 1, 0])\n",
    "        if val >= 2:\n",
    "            signal = make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            for i in range(val - 1):\n",
    "                signal += make_signal(2 * np.pi * np.random.randint(0, N) / N, 0, m)\n",
    "            signals.append(signal + make_noise(sigma2, m))\n",
    "            freqs.append([0, 0, 1])\n",
    "    return signals, freqs\n",
    "\n",
    "def get_sigma2_from_snrdb(SNR_db):\n",
    "    return 10**(-SNR_db/10)\n",
    "\n",
    "def kay_weights(N):\n",
    "    scaling = (3.0/2)*N/(N**2 - 1)\n",
    "    \n",
    "    w = [1 - ((i - (N/2 - 1))/(N/2))**2 for i in range(N-1)]\n",
    "    \n",
    "    return scaling*np.array(w)\n",
    "\n",
    "def kays_method(my_signal):\n",
    "    N = len(my_signal)\n",
    "    w = kay_weights(N)\n",
    "    \n",
    "    angle_diff = np.angle(np.conj(my_signal[0:-1])*my_signal[1:])\n",
    "    need_to_shift = np.any(angle_diff < -np.pi/2)\n",
    "    if need_to_shift:    \n",
    "        neg_idx = angle_diff < 0\n",
    "        angle_diff[neg_idx] += np.pi*2\n",
    "    \n",
    "    return w.dot(angle_diff)\n",
    "\n",
    "def kays_singleton_accuracy(test_signals, test_freqs, N):\n",
    "    diffs = [s - make_signal(kays_method(s), 0, N) for s in test_signals]\n",
    "    thresh, single_acc, other_acc, best_thresh = 0.0, 0, 0, 0\n",
    "    best = 0\n",
    "    for i in range(150):\n",
    "        vals = [(sum(np.absolute(s)) / N) < thresh for s in diffs]\n",
    "        corr = [1 for i in range(len(test_freqs)) if (test_freqs[i] == [0, 1, 0] and vals[i] == 1) or ((test_freqs[i] != [0, 1, 0] and vals[i] == 0))]\n",
    "        corr = sum(corr)\n",
    "        #single = sum([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] == [0, 1, 0]])\n",
    "        #other = sum([not vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]]) / len([vals[d] for d in range(len(vals)) if test_freqs[d] != [0, 1, 0]])        \n",
    "        #if single*2 + other > single_acc*2 + other_acc and single > 0.2 and other > 0.2:\n",
    "        #    single_acc = single\n",
    "        #    other_acc = other\n",
    "        #    best_thresh = thresh\n",
    "        if corr > best:\n",
    "            best = corr\n",
    "            best_thresh = thresh\n",
    "        thresh += 0.05\n",
    "    print('thresh: ', best_thresh)\n",
    "    return best / len(test_signals)\n",
    "\n",
    "def test_kays(signals, freqs, N):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        res = kays_method(sig)\n",
    "        res = round(res * N / (2 * np.pi))\n",
    "        if np.argmax(freq) == res:\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "\n",
    "def test_mle(signals, freqs, N, m):\n",
    "    count = 0\n",
    "    for sig, freq in zip(signals, freqs):\n",
    "        cleans = [make_signal(np.pi * 2 * w / N, 0, m) for w in range(N)]\n",
    "        dots = [np.absolute(np.vdot(sig, clean)) for clean in cleans]\n",
    "        if np.argmax(dots) == np.argmax(freq):\n",
    "            count += 1\n",
    "    return count / len(signals)\n",
    "    \n",
    "def make_binary(freqs, N):\n",
    "    w = math.ceil(np.log2(N))\n",
    "    return [[int(a) for a in list(np.binary_repr(f, width=w))] for f in freqs] \n",
    "\n",
    "def binary_to_int(binary_string):\n",
    "    return tf.reduce_sum(\n",
    "    tf.cast(tf.reverse(tensor=binary_string, axis=[0]), dtype=tf.int64)\n",
    "    * 2 ** tf.range(tf.cast(tf.size(binary_string), dtype=tf.int64)))\n",
    "    '''y = 0\n",
    "    for i,j in enumerate(x):\n",
    "        y += j<<i\n",
    "    return y'''\n",
    "\n",
    "def hamming(pred, act):\n",
    "    return np.count_nonzero(pred != act)\n",
    "\n",
    "def one_hot(N, batch_size, freqs):\n",
    "    freqs_one_hot = np.zeros((batch_size, N))\n",
    "    freqs_one_hot[np.arange(batch_size), freqs] = 1\n",
    "    return freqs_one_hot\n",
    "\n",
    "def test_noisy_mle(N, m, signals, freqs):\n",
    "    count = 0  \n",
    "    '''imag_signals = []\n",
    "    for index in range(len(signals)):\n",
    "        sig = signals[index]\n",
    "        imag_sig = [(sig[i] + 1j*sig[i+1]) for i in np.arange(len(sig), step=2)]\n",
    "        imag_signals.append(imag_sig)'''\n",
    "    cleans = [make_signal(2*np.pi*i/N, 0, m) for i in range(N)]\n",
    "                     \n",
    "    for index in range(len(signals)):\n",
    "        dots = [np.absolute(np.vdot(signals[index], cleans[i])) for i in range(N)]\n",
    "        if np.argmax(freqs[index]) == np.argmax(dots):\n",
    "            #print(np.argmax(dots))\n",
    "            count += 1\n",
    "    return count / len(freqs)\n",
    "\n",
    "def bit_to_freq(bits, N):\n",
    "    possible = [i for i in range(N)]\n",
    "    for b in bits:\n",
    "        if b[0]:\n",
    "            possible = possible[:len(possible)//2]\n",
    "        else:\n",
    "            possible = possible[len(possible)//2:]\n",
    "    return possible[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n",
      "Iter 500, Minibatch Loss= 0.6953, Training Accuracy= 0.527\n",
      "Iter 1000, Minibatch Loss= 0.6822, Training Accuracy= 0.546\n",
      "Iter 1500, Minibatch Loss= 0.5367, Training Accuracy= 0.750\n",
      "Iter 2000, Minibatch Loss= 0.3538, Training Accuracy= 0.865\n",
      "Iter 2500, Minibatch Loss= 0.2788, Training Accuracy= 0.896\n",
      "Iter 3000, Minibatch Loss= 0.3141, Training Accuracy= 0.877\n",
      "Iter 3500, Minibatch Loss= 0.2786, Training Accuracy= 0.892\n",
      "Iter 4000, Minibatch Loss= 0.3226, Training Accuracy= 0.881\n",
      "Iter 4500, Minibatch Loss= 0.2746, Training Accuracy= 0.900\n",
      "Iter 5000, Minibatch Loss= 0.2887, Training Accuracy= 0.885\n",
      "Iter 5500, Minibatch Loss= 0.2626, Training Accuracy= 0.900\n",
      "Iter 6000, Minibatch Loss= 0.1742, Training Accuracy= 0.954\n",
      "Iter 6500, Minibatch Loss= 0.2511, Training Accuracy= 0.912\n",
      "Iter 7000, Minibatch Loss= 0.3819, Training Accuracy= 0.842\n",
      "Iter 7500, Minibatch Loss= 0.3306, Training Accuracy= 0.869\n",
      "Iter 8000, Minibatch Loss= 0.2386, Training Accuracy= 0.919\n",
      "Iter 8500, Minibatch Loss= 0.1881, Training Accuracy= 0.946\n",
      "Iter 9000, Minibatch Loss= 0.0161, Training Accuracy= 0.992\n",
      "Iter 9500, Minibatch Loss= 0.0117, Training Accuracy= 0.996\n",
      "Iter 10000, Minibatch Loss= 0.0155, Training Accuracy= 0.996\n",
      "Iter 10500, Minibatch Loss= 0.0071, Training Accuracy= 0.996\n",
      "Iter 11000, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Iter 11500, Minibatch Loss= 0.0040, Training Accuracy= 1.000\n",
      "Iter 12000, Minibatch Loss= 0.0079, Training Accuracy= 0.996\n",
      "Iter 12500, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "Iter 13000, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "Iter 13500, Minibatch Loss= 0.0205, Training Accuracy= 0.996\n",
      "Iter 14000, Minibatch Loss= 0.0214, Training Accuracy= 0.996\n",
      "Iter 14500, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Iter 15000, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Iter 15500, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Iter 16000, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "Iter 16500, Minibatch Loss= 0.0051, Training Accuracy= 0.996\n",
      "Iter 17000, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 17500, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "Iter 18000, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "Iter 18500, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "Iter 19000, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Iter 19500, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 20000, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "Iter 20500, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "Iter 21000, Minibatch Loss= 0.0101, Training Accuracy= 0.996\n",
      "Iter 21500, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "Iter 22000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 22500, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Iter 23000, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Iter 23500, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 24000, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Iter 24500, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Iter 25000, Minibatch Loss= 0.0169, Training Accuracy= 0.996\n",
      "Iter 25500, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "Iter 26000, Minibatch Loss= 0.0034, Training Accuracy= 0.996\n",
      "Iter 26500, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Iter 27000, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Iter 27500, Minibatch Loss= 0.0161, Training Accuracy= 0.996\n",
      "Iter 28000, Minibatch Loss= 0.0035, Training Accuracy= 0.996\n",
      "Iter 28500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 29000, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "Iter 29500, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Iter 30000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 30500, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "Iter 31000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 31500, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 32000, Minibatch Loss= 0.0058, Training Accuracy= 0.996\n",
      "Iter 32500, Minibatch Loss= 0.0309, Training Accuracy= 0.992\n",
      "Iter 33000, Minibatch Loss= 0.0053, Training Accuracy= 0.996\n",
      "Iter 33500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 34000, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Iter 34500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 35000, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 35500, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 36000, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Iter 36500, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 37000, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 37500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 38000, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "Iter 38500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 39000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 39500, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "Iter 40000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 40500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 41000, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Iter 41500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 42000, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Iter 42500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 43000, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 43500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 44000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 44500, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "Iter 45000, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 45500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 46000, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "Iter 46500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 47000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 47500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 48000, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Iter 48500, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 49000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 49500, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "Iter 50000, Minibatch Loss= 0.0649, Training Accuracy= 0.985\n",
      "Iter 50500, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 51000, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 51500, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "Iter 52000, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Iter 52500, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "Iter 53000, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 53500, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "Iter 54000, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "Iter 54500, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Iter 55000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 55500, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 56000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 56500, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Iter 57000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 57500, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "Iter 58000, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Iter 58500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 59000, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "Iter 59500, Minibatch Loss= 0.0401, Training Accuracy= 0.992\n",
      "Iter 60000, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 60500, Minibatch Loss= 0.0036, Training Accuracy= 0.996\n",
      "Iter 61000, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 61500, Minibatch Loss= 0.0244, Training Accuracy= 0.996\n",
      "Iter 62000, Minibatch Loss= 0.0050, Training Accuracy= 0.996\n",
      "Iter 62500, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 63000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 63500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 64000, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Iter 64500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 65000, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Iter 65500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 66000, Minibatch Loss= 0.0069, Training Accuracy= 1.000\n",
      "Iter 66500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 67000, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "Iter 67500, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 68000, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "Iter 68500, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Iter 69000, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Iter 69500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 70000, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "Iter 70500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 71000, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "Iter 71500, Minibatch Loss= 0.0689, Training Accuracy= 0.988\n",
      "Iter 72000, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "Iter 72500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 73000, Minibatch Loss= 0.0340, Training Accuracy= 0.988\n",
      "Iter 73500, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "Iter 74000, Minibatch Loss= 0.0029, Training Accuracy= 1.000\n",
      "Iter 74500, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "Iter 75000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 75500, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Iter 76000, Minibatch Loss= 0.0134, Training Accuracy= 0.996\n",
      "Iter 76500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Iter 77000, Minibatch Loss= 0.0175, Training Accuracy= 0.992\n",
      "Iter 77500, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "Iter 78000, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "Iter 78500, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Iter 79000, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "Iter 79500, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Iter 80000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "Training Finished\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99615383, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# div and conquer freq detect\n",
    "\n",
    "N = 8192 \n",
    "#SNRdB = -2\n",
    "layer = 6\n",
    "m = 40\n",
    "\n",
    "log = int(np.log2(N))\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "num_iter = 80000\n",
    "batch_size = log * 20\n",
    "\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_classes = 1\n",
    "\n",
    "#ms = [20, 30, 50, 80, 120, 200]\n",
    "snrs = [10, 8, 6, 4, 2, 0]\n",
    "\n",
    "bin_accs = []\n",
    "freq_accs = []\n",
    "\n",
    "\n",
    "for SNRdB in snrs:\n",
    "\n",
    "    t_bins = []\n",
    "    t_freqs = []\n",
    "    for trial in range(4):\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, m, 2])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        # Store layers weight & bias\n",
    "        weights = {i: tf.Variable(tf.random_normal([3, 2, 2])) for i in range(1, layer+1)}\n",
    "        weights[0] = tf.Variable(tf.random_normal([5, 2, 2]))\n",
    "        weights['out'] = tf.Variable(tf.random_normal([(m-4-(2*layer))*2, num_classes]))\n",
    "        biases = {i: tf.Variable(tf.random_normal([2])) for i in range(layer+1)}\n",
    "        biases['out'] = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights[0], 1, 'VALID'), biases[0])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            for i in range(1, layer+1):\n",
    "                layer_1 = tf.add(tf.nn.conv1d(hidden_1, weights[i], 1, 'VALID'), biases[i])\n",
    "                hidden_1 = tf.nn.relu(layer_1)\n",
    "            hidden_3 = tf.reshape(hidden_1, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "\n",
    "\n",
    "        '''weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([5, 2, 2])), # filtersize, in channels, outchannels\n",
    "            'out': tf.Variable(tf.random_normal([(m-4-2-2)*2, num_classes])),\n",
    "            'h2': tf.Variable(tf.random_normal([3, 2, 2])),\n",
    "            'h3': tf.Variable(tf.random_normal([3, 2, 2]))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([2])),\n",
    "            'out': tf.Variable(tf.random_normal([num_classes])),\n",
    "            'b2': tf.Variable(tf.random_normal([2])),\n",
    "            'b3': tf.Variable(tf.random_normal([2]))\n",
    "        }\n",
    "\n",
    "        def neural_net(x):\n",
    "            layer_1 = tf.add(tf.nn.conv1d(x, weights['h1'], 1, 'VALID'), biases['b1'])\n",
    "            hidden_1 = tf.nn.relu(layer_1)\n",
    "            layer_2 = tf.add(tf.nn.conv1d(hidden_1, weights['h2'], 1, 'VALID'), biases['b2'])\n",
    "            hidden_2 = tf.nn.relu(layer_2)\n",
    "            layer_3 = tf.add(tf.nn.conv1d(hidden_2, weights['h3'], 1, 'VALID'), biases['b3'])\n",
    "            hidden_3 = tf.nn.relu(layer_3)\n",
    "            hidden_3 = tf.reshape(hidden_3, [batch_size, -1])\n",
    "            out_layer = tf.matmul(hidden_3, weights['out']) + biases['out']\n",
    "            return out_layer\n",
    "        '''\n",
    "        test_dict = {}\n",
    "        for i in range(10):\n",
    "            test_signals, test_freqs, freqs = make_batch_noisy_lohi(batch_size // log, SNRdB, N, m)\n",
    "            test_signals_pair = np.zeros((batch_size, m, 2))\n",
    "            test_signals_pair[:, :, 0] = np.real(test_signals)\n",
    "            test_signals_pair[:, :, 1] = np.imag(test_signals)\n",
    "            test_dict[i] = (test_signals_pair, test_freqs, freqs)\n",
    "\n",
    "\n",
    "        training_size = 3999\n",
    "        dict = {}\n",
    "        for i in range(training_size):\n",
    "            batch_x, batch_y, batch_freqs = make_batch_noisy_lohi(batch_size // log, SNRdB - (i % 3), N, m)\n",
    "            batch_x_pair = np.zeros((batch_size, m, 2))\n",
    "            batch_x_pair[:, :, 0] = np.real(batch_x)\n",
    "            batch_x_pair[:, :, 1] = np.imag(batch_x)\n",
    "            dict[i] = (batch_x_pair, batch_y)\n",
    "\n",
    "\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net(X)\n",
    "        prediction = tf.nn.sigmoid(logits)\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))  \n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "        # Evaluate model\n",
    "        pred_class = tf.greater(prediction, 0.5)\n",
    "        correct_pred = tf.equal(pred_class, tf.equal(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "            print(\"Training Started\")\n",
    "\n",
    "            for step in range(1, num_iter + 1):\n",
    "                batch_x_pair, batch_y = dict[step % training_size]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x_pair, Y: batch_y})\n",
    "                if step % 500 == 0:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc, pred = sess.run([loss_op, accuracy, prediction], feed_dict={X: batch_x_pair,\n",
    "                                                                         Y: batch_y})\n",
    "\n",
    "                    accuracies.append(acc)\n",
    "                    losses.append(loss)\n",
    "                    print(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "            print(\"Training Finished\")\n",
    "            preds = [sess.run(prediction, feed_dict={X: test_dict[i][0], Y: test_dict[i][1]}) for i in range(len(test_dict))]  \n",
    "            nn_acc = [sess.run(accuracy, feed_dict={X: test_dict[i][0], Y: test_dict[i][1]}) for i in range(len(test_dict))]   \n",
    "            print(nn_acc)\n",
    "\n",
    "        t_bins.append(np.median(nn_acc))\n",
    "        preds = np.round(preds)\n",
    "        corr = []\n",
    "        for a in range(len(test_dict)):\n",
    "            fs = []\n",
    "            for k in range(len(preds[a]) // log):\n",
    "                    fs.append(bit_to_freq(preds[a][k * log : (k+1) * log], N))\n",
    "            corr.extend([fs[i] == test_dict[a][2][i] % N for i in range(len(fs))])\n",
    "        t_freqs.append(np.sum(corr) / (len(fs) * len(test_dict)))\n",
    "    bin_accs.append(max(t_bins))\n",
    "    freq_accs.append(max(t_freqs))\n",
    "    print(bin_accs[-1])\n",
    "    print(freq_accs[-1])\n",
    "    \n",
    "np.save('./data/divide_conquer/snrs_8192', snrs)   \n",
    "np.save('./data/divide_conquer/snr_acc_binary_8192', bin_accs)\n",
    "np.save('./data/divide_conquer/snr_acc_frequency_8192', freq_accs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34\n"
     ]
    }
   ],
   "source": [
    "preds = np.round(preds)\n",
    "corr = []\n",
    "for a in range(len(test_dict)):\n",
    "    fs = []\n",
    "    for k in range(len(preds[a]) // log):\n",
    "            fs.append(bit_to_freq(preds[a][k * log : (k+1) * log], N))\n",
    "    corr.extend([fs[i] == test_dict[a][2][i] % N for i in range(len(fs))])\n",
    "print(np.sum(corr) / (len(fs) * len(test_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
